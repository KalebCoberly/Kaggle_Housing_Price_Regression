---
title: "EDA and Feature Engineering for Housing Price Regression"
author: "Kaleb Coberly"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
    number_sections: true
    code_folding: show
  html_notebook: default
  pdf_document: default
---

<a id="top"></a>

# Overview

In this document, I wrangle, explore, and engineer data in preparation for statistical and machine learning (ML) regression of county assessor data to predict residential sales prices. I will use a few regression models to test whether I added value with my treatment of the data. If the selected ML algorithms build models that perform significantly and meaningfully better with my engineered data than with selected "raw" data, then I can say that I have added value to the data by making it more amenable to ML.

The exploratory data analysis (EDA) and feature engineering process (excluding preprocessing steps like min-max scaling) are outlined and explained in a fair amount of detail in this overview section. The narrative, code, stats, and visualizations in the rest of the document and associated scripts should be viewed as content that can be referred to or used for other polished documents and production processes, not as polished products themselves.

## Modeling and Hypothesis Test

[Back to top.](#top)

I will build the models and conduct hypothesis tests in a following notebook. I will compare the prediction errors of the models trained on the "raw" control data to the prediction errors of the models trained on the experimental engineered data.

*Null hypothesis:* Feature engineering did not lower mean errors.

*Alternative hypothesis:* Feature engineering lowered mean errors.

I will pool the errors within each group rather than grouping by ML algorithm and running a separate hypothesis test for each algorithm. This will avoid "p-hacking" with multiple tests.

If the errors warrant it (e.g., they are normally distributed or can be made to be normally distributed), I will use a simple one-tailed [Student's t-test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test){target="_blank"}. If not, I will use a [Wilcoxon rank-sum test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test){target="_blank"} (if warranted), which is a stochastic hypothesis test that is valid for data that is not normally distributed. Pooling the errors of multiple models will likely make the distribution polymodal, so the rank-sum test will probably be necessary.

### Scaling Errors for Training vs. Comparison

I could apply a log10 transformation to sales prices in order to scale errors proportionally to the sales prices, which would avoid overweighting errors on pricier houses. This would have the added benefit of making the target variable more normally distributed and thus better fit to linear regression and ML algorithms that perform better with normally distributed variables.

However, I found that a natural logarithm better normalized the target variable in the training set, according to the results of a [Shapiro-Wilk normality test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test){target="_blank"}. So, I will need to divide the errors by the values (error/log(SalePrice)), making errors a percentage. Rather than go with the sensible log10 scale anyway, I will use this opportunity to get "under the hood" of the ML algorithms a little bit and write my own summary function to adjust the errors.

I will use the Root Mean Squared Error (RMSE) of these adjusted errors in model training and evaluation with both the control data and the experimental data. Taking the squared errors penalizes greater errors (though not pricier errors, since the errors are measured as a percentage). Taking the square root of the mean re-standardizes the distance between model mean errors for a "flatter" comparison of models in training.

To compare the experimental errors to the control errors for the hypothesis test, I will reverse the log transformation of the experimental predictions. I will then divide the errors by the true target value for both groups.

Though I will pool errors to conduct a single test of my overall hypothesis, I may also use a few hypothesis tests to explore the results. In that case, I will only conduct a hypothesis test between like ML models. For instance, I will test how the results of a Random Forest (RF) model trained on the control data compare to the results of a RF model trained on the experimental data. But, I will not test an RF model against a K-Nearest-Neighbors (KNN) model. I will, of course, generally compare models created with different algorithms, just not as a hypothesis test.

### Model and Feature Selection

In preparation for this project, I interviewed a home appraiser. I wanted to find out which features appraisers typically consider and get a sense of their analytical methods. This will help me construct a control data set and select test ML algorithms.

Based on that discussion, I made a list of features to use to train the control models. Other than basic auditing/wrangling and scaling, I will use these features as is. The results of the exploration and feature engineering in this document will feed into selection of the experimental data set.

The point of this exercise is to measure how well I prepare data for ML and statistical use, not to attempt to recreate what a human appraiser does. That said, I identified three ML algorithms that sort of emulate what an appraiser does.

I selected RF because a decision tree groups like observations as an appraiser might group like houses to generate comparisons ("comps"). Also, a decision tree doesn't rely on standardized data, so it puts the control data and the engineered data on more level ground.

KNN also clusters observations into data "neighborhoods" in a similar way that appraisers run comps. While I may min-max scale the control data for KNN to give it sensible distance measures, I do not see the lack of normalization to necessarily be a disadvantage to the control data in KNN. Outliers may aid intelligent clustering in a multivariate problems. For this reason, I may choose to use some less-transformed features in the experimental set for KNN in an additional informal test to explore this idea.

Lastly, Lasso regression intelligently avoids overfit by penalizing each additional feature. This more closely emulates what a human appraiser does by adding and subtracting value with each feature. The engineered data will be at a distinct advantage, however, as I have engineered features with an eye to linear regression (mainly normalizing/Winsorizing continuous variables and identifying insignificant factor levels).

I both transformed and Winsorized the scale of the target variable, SalePrice. I will apply the transformation to the target variable in the test set, but not Winsorization as it is not a process you can reverse in a vectorized fashion. I will train the Lasso regression with the Winsorized target to achieve an optimal fit, but test without Winsorizing the target.

I will use [caret](https://cran.r-project.org/web/packages/caret/index.html){target="_blank"} for ML, preprocessing (min-max scaling, imputation), and feature selection. I will keep preprocessing minimal (e.g., forgo decomposition methods like principal component analysis) as I want to test the efficacy of the "manual" engineering I have done before such steps.

I may also construct a multilinear regression "by hand" with the R [stats](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html){target="_blank"} package, which will allow me to explicitly create variable interactions.

## EDA and Feature Engineering

[Back to top.](#top)

In this document, I audit and wrangle, split, explore, and engineer the data.

### Auditing and Wrangling

Though I do not document the auditing process, the wrangling script ("tools/wrangle.R") I built as a result documents the treatments I made to the data before splitting and exploring it. This treatment mainly consists of setting data types, standardizing factor levels, checking for internal consistencies, and imputing missing values that can logically be deduced. I will impute remaining missing values using caret in the ML phase.

The script should work on future data (e.g. the test set), though some issues may emerge that are idiosyncratic to the new set. I will test it out when I move on to ML. That said, I did use the wrangling script to create summary objects that aided the auditing process. As such, the script constructs and returns a lot of unnecessary data; it can certainly be trimmed and refactored.

### Train-Test Split

I split the data into training, validation, and testing sets. Even though I will use cross-validation while training the models, I set aside a validation set so that I would have ample space to explore and try out ideas before testing. I used a 70-30 split in two rounds so that the testing set is 30% of the set I have to work with, the validation set is 21% (0.3 \* 0.7), and the training set is 49% (0.7 \* 0.7).

I was especially concerned with representation across all variables. A random sample of the index did not ensure a representative sample. The more features you have, the more confident you can be that one or more of them are *not* representative with a random split. I found methods to stratify, but only by a single discrete variable at a time, which also did not produce representative splits.

To test this, I used the [Wilcoxon rank-sum test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test){target="_blank"} to compare the split subsets for each continuous feature. This as a non-parametric test and is thus suited to testing data that is not guaranteed to be normalized, as was the case before exploration and engineering. It simply finds the distance between each point in one set and each point in the other set. Then it uses a parametric test to determine if the mean distance differs significantly from 0. Because the objective is to ensure representation and the null hypothesis of this test is that the sets do not differ signficantly, we want to fail to reject the null hypothesis. The criterion, then, is a p-value greater than alpha, which ought to be set high to have higher confidence in the representativity of the split.

From this, I built a naive data set splitter (in "tools/split.R") that randomly samples observations then applies the Wilcoxon rank-sum test to each continuous feature. Should one feature fail to meet the criterion for representativity, the script resamples and tries again.

This is unabashedly aggressive "p-hacking" or "data dredging," but that is not actually a problem here. We already know the subsets come from the same superset, and we are not concerned with being confident one way or another about this hypothesis. The objective is simply to create subsets that *appear* to come from the same superset, subsets that are *representative*.

That said, this process can run for a very long time, infinitely in some cases. As with the wrangling script, I built in some extraneous processes that helped monitor the process, and it certainly can be refactored for better performance. Refactoring it will not really mitigate the underlying issue of brute force -- worse than brute force as random samples can repeat. You could modify it to return the "best" split after a given amount of time running.

I sought another method and found the maximum dissimilarity method in caret, [maxDissim](https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/maxDissim){target="_blank"}. This elegant solution iteratively selects points that are most dissimilar to the sample set, running in much less time.

It did, however, produce worse results than my method, according to a Wilcoxon rank-sum test. [See results below.](#comp_by_wilcox) But, that is a "rigged" scenario because it applies the same test that I p-hacked to create my subset.

I instead implemented an adversarial validation using [caret's xgbTree](http://topepo.github.io/caret/available-models.html){target="_blank"}. In adversarial validation, you label each set in the split and see if the algorithm can tell the difference. If accuracy is 0.5, the model cannot tell the difference between one half of the split and the other, which is a good indication that your split is representative.

My split method produced a much more representative split than maxDissim. Splitting again for the validation set produced results close to maxDissim, though I did not split again with maxDissim to compare. The Kaggle split far outperformed my method and maxDissim. That said, Kaggle used a 50-50 split, whereas I used a 70-30 split, which may account somewhat for the difference in accuracy variance and mean. Though they have kept their method hidden as far as I can tell, I am guessing they incorporated multifactor stratification to do so well with a classification tree. [See results below.](#comp_by_AV)

Both maxDissim and my method only use the continuous variables. Thus, representation is undoubtedly still inaccurate in at least some factors. An alternative method may incorporate recursive stratification.

### Exploratory Data Analysis (EDA) and Feature Engineering

I mostly focused my exploration on predicting the target variable, SalePrice. I developed a standard process for handling different kinds of variables, both to engineer them and to visualize them.

I duplicated the engineering process in a well-documented engineering script ("tools/engineer.R") to apply to future data. I also developed a well-documented source file ("tools/eda.R") of the EDA tool kit that I used in this document. The engineering script could definitely be merged and refactored with the wrangling script for optimal computation. I will also need to verify that the engineering script produces the same output as this notebook, which I will do in the next notebook.

While I focused mostly on basic regression prep, I did explore some interactions between predictor variables, and I did tell some "side stories." A few examples: examples [here](#neighbZoning), [here](#totbathBsmtbath), [here](#garbltControl), and [here](#saletypeDate).

#### Continuous Variables

For continuous variables, I sought the best normalizing scale transformation and Winsorization. In doing so, I prioritized correlations to SalePrice. I skipped multivariate Winsorization, which would have aided straightforward linear regression but may have hindered clustering and interactions between variables. I may choose to do targeted multivariate Winsorization in the next phase as I select and preprocess the variables for each different ML algorithm.

I iterated through a list of potential transformation functions and used [Shapiro-Wilk normality test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test){target="_blank"} to determine which function best normalized the variable. Again, this was a valid form of p-hacking in that I was unconcerned about the normality of ground truth and only needed the quality of normality in the variables. However, I naively excluded 0 values to avoid throwing an error logarithmic transformations, but that still transforms 1s into 0s, rendering logs useless for variables with a large number of 1s. Fortunately, there were other transformation functions. I did not implement more dynamic transformation functions like Box-Cox and Yeo-Johnson.

I also manually estimated the best Winsorization using the same normality test and a QQ plot. I hardcoded the upper and lower limit values into the engineering script, rather than using the percentiles, to avoid leakage into the test set.

I could have automated this process more, but I wanted to open it up and inspect it to convey a better sense for what I was doing and whether it was a sensible thing to do. I may also have developed a more robust measure of normality, including more metrics like kurtosis.

For variables in which a 0 indicates a missing feature, I normalized only the non-zero set. The idea was that it might aid regression when the variable is put into interaction with its missingness.

I also determined whether simply binarizing the presence of a feature would better correlate to sale price than a linear regression of the full variable.

To verify that my transformations and modifications produced sensible outcomes, I visualized at each step.

As mentioned above, I applied a natural log to SalePrice because it "best" normalized the variable, whereas a log10 transformation would have been more sensible. I will mitigate this when I write a custom summary function to train with. But, similarly, for other variables a more sensible transformation might have wiser to use than the one I used. For instance, measures of area may most sensibly be transformed to their square-root, but a different function have "best" normalized the data in the training set. I went with the function that my algorithm spit out here, but I may return to the question in the ML phase and compare sensible transformations to "optimal" transformations.

In some cases, my treatment of the data may have constituted overfitting (e.g. log(sqrt(variable))), but I often went with it as I am not concerned as much with the best outcome as with seeing what happens when conventional wisdom is ignored. I will pay attention in the next phase of modeling to whether those variables are more duds than their training set correlations implied they might be.

#### Factors

As with continuous variables, I developed a standard approach to exploring factors. It was decidedly less involved exploration and engineering.

For each factor, I summarized sale price by factor levels, both in table form and in a plot. I also automated iterative hypothesis testing for significant differences in sale prices between levels, identifying levels that differed significantly than another level. This was only for exploration purposes and should not be used to model, as it is an invalid form of p-hacking. I chose not to calculate effect size for this reason, and because I visualized it to some extent with notched boxplots.

One-hot encoding the factors and then intelligently selecting features in the ML phase will suss this out anyway. At that point, I will also determine if simply binarizing the presence of a feature better predicts price.

#### Integers and Ordered Factors

I treated integers and ordered factors as I did with both continuous variables and factors.

## Background

[Back to top.](#top)

I downloaded the data from [this Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques){target="_blank"}. Kaggle acquired the set from a university professor who discussed the set and his treatment of it in [this article](http://jse.amstat.org/v19n3/decock.pdf){target="_blank"}. He cleaned up the set quite a bit. For instance, he removed the city's modeling variables, restricted records to residential sales, and removed previous sales of the same home within the given period.

## Technical Choices

[Back to top.](#top)

I wanted to do this project completely in R because I want to try out the caret package for the ML. Also, it is a forgiving enough data set that I do not need to do too much wrangling, so there is no need to use Python when R is so fit for analysis and visualization. The light amount of wrangling is a good opportunity to get more comfortable with R beyond ggplot2.

I am also choosing to do it completely on my PC in R-Studio because I like the robust features of R-Studio and because I want to get a sense of my computer's performance. Some of the compute, like ML training, may be best done on a cloud server, but that will be another project.

### A Note on the Code

Much of the code in this document is repeated and could be wrapped into functions, but I chose to leave it exposed and paste it in for each variable so I could easily play with it with each use. Likewise, many of the functions in the scripts wrap a lot of now-extraneous processes and objects that were useful as I inspected the processes while I built them. These could be stripped down. The "tools/engineer.R" script is pretty streamlined, though it could be combined with the "tools/wrangle.R" script and refactored.

## References

Dean De Cock:[http://jse.amstat.org/v19n3/decock.pdf](http://jse.amstat.org/v19n3/decock.pdf){target="_blank"}

Kaggle: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques){target="_blank"}

Shapiro-Wilk normality test: [https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test){target="_blank"}

Student's t-test: [https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test){target="_blank"}

Wilcoxon rank-sum test: [https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test){target="_blank"}

[See loaded packages below.](#libraries)

# Diving In

[Back to top.](#top)

## Notes on Data Dict

Are there a lot of duplexes and 2-family conversions? Can/should I ignore them? What about agricultural/commercial/industrial? Floating Village Residential?

Alley NA codes no alley access, not missing value.

LandContour and LandSlope are similar. BlgdType, HouseStyle, MSSubClass also overlap.

Condition1/Condition2 have interesting relationship.

YearRemodAdd same as YearBuilt if no remodel.

Some features only have values conditional to other values (e.g. Cond1/Cond2, Masonry veneer type, etc.)

BsmtQual is actually height

Proportion of beds/baths/kitchens above grade might interact with number of said rooms.

SaleType and SaleCondition will probably be significant

## Libraries and Source Files

<a id="libraries"></a>

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
# library(dict) # Still not found after installation
library(container) # For Dict class
library(useful) # For simple.impute
library(comprehenr) # For list comprehension
library(GGally)
library(reshape2)
library(gridExtra)
library(gplots)
library(DescTools) # For df summary
library(robustHD) # For df summary
library(caret)
library(effsize) # For Cohen's d

source('tools/wrangle.R')
source('tools/eda.R')
source('tools/engineer.R')
source('tools/split.R')

# SEED = 65466
```

## Loading Data

Here's the set straight from Kaggle.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
kaggle_train_set = read.csv(file = 'data/train.csv')
kaggle_test_set = read.csv(file = 'data/test.csv')

# str(kaggle_train_set)
```

# Wrangling

[Back to top.](#top)

Below runs a wrangling script I built for this set. First, I removed duplicates. Then, I set the feature data types and standardized the factor levels. Then, I explicitly set missing values to a missingness value where it is known, assumed, or implied (by related features) to indicate a missing feature in reality rather than a missing value in the data. Finally, I checked some logic between features, enforcing some rules.

For instance, I dropped a couple of records in which the house style was explicitly one-story but for which there was a value for second-story square footage. I was confident that square footage would be an important predictive feature, so it was worth dropping the records to remove noise from these features. In similar cases where the logic of feature values didn't add up but I wasn't sure I would even end up keeping the features, I made a note to remove the records from the training set if the features turn out to be important. Specifically, I'll remove the one record where land contour is level but land slope is severe and the five records where masonry veneer type is qualified but masonry veneer square footage is 0 or missing, but only if those features are worth keeping.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# source('tools/wrangle.R')
wrangle_lst = wrangle(df = kaggle_train_set)
kaggle_train_set = wrangle_lst$df
wrangle_lst = wrangle(df = kaggle_test_set)
kaggle_test_set = wrangle_lst$df
rm(wrangle_lst)
```

# Split

[Back to top.](#top)

## Validate Kaggle Split

Are the train and test sets representative of the same population? That is, is the train set adequate to train a model to test on the test set? I'll validate with two methods: Wilcoxon rank-sum test and adversarial validation.

### Wilcoxon rank-sum test

This as a non-parametric test and is thus suited to testing data that is not guaranteed to be normalized, as is the case before exploration and engineering. It simply finds the distance between each point in one set and each point in the other set. Then it uses a parametric test to determine if the mean distance differs significantly from 0. Because the objective is to ensure representation and the null hypothesis of this test is that the sets do not differ signficantly, we want to fail to reject the null hypothesis. The criterion, then, is a p-value greater than alpha, which ought to be set high to have higher confidence in the representativity of the split.

```{r echo=TRUE, message=FALSE, warning=FALSE}
double_feats_lst = colnames(select(kaggle_train_set, where(is.double)))

Kag_validate_split_lst = validate_split(
  train = kaggle_train_set,
  test = kaggle_test_set,
  feats_lst = double_feats_lst,
  y_cols = list('SalePrice'),
  alpha = 0,
  target_alpha = 0
)

Kag_validate_split_lst
```

Using the Wilcoxon rank-sum test to estimate how well each continuous variable in the train set seems to represent the same superset as the corresponding variable in the test set, we find that most features do not pass the test. Because we're after no significant difference, we want higher p-values to ensure better representation. Only MiscVal has p > 0.9 (90% chance of incorrectly rejecting the null that these sets represent the same population), whereas three variables have p < 0.1, very probably not representative of the same set. Only 8 out of the 20 tested variables even have p > 0.5.

### Adversarial Validation

In adversarial validation, you label each set in the split and see if the algorithm can tell the difference. If accuracy is 0.5, the model cannot tell the difference between one half of the split and the other, which is a good indication that your split is representative.

We can inspect the results of about 100 different models with XGBoost. They all seem to have trouble differentiating between Kaggle's train and test sets, a good indication that the split is representative. We'll return to visualize this against the splits I create.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# # Uncomment to revalidate.
# kag_split_av = adversarial_validation(
#   train = select(kaggle_train_set, -c('Id', 'SalePrice')),
#   test = select(kaggle_test_set, -c('Id'))
# )
# saveRDS(kag_split_av, 'data/kag_split_av.rds')

kag_split_av = readRDS('data/kag_split_av.rds')

summary(kag_split_av$results$Accuracy)
```

## Train-Test Split

[Back to top.](#top)

For the purposes of this project, I treat the Kaggle training set as the full set to use for training and testing, because I won't get access to the Kaggle test set. So, I make a 70-30 train-test split from the Kaggle training set, then I make another 70-30 train-validate split from the new training set.

I'll later use cross-validation to tune the algorithm, so a validation set isn't technically necessary. But, having a validation set allows me to try more than one algorithm. There is enough data to make the extra split, and once I select a tuned model, I can retrain it on the full training set before testing it on the final test set. (And, for the Kaggle competition, I can repeat the process once again to build my final submission.)

### maxDissim vs. Random Search with Wilcoxon Rank-Sum

How to make sure each feature of the split is optimally representative of the superset? I tried two different methods to split: caret's maxDissim, and a random search with the Wilcoxon rank-sum test to validate across multiple variables.

#### maxDissim

I tried using caret's `maxDissim` for the split. It's an elegant approach to this problem. Simply take a small random sample to create a seed set, then find the observation in the superset that is most dissimilar to the seed set and add it to the seed set. Repeat until the seed set has grown to the size you want it.

But, how do we know it worked? As we have not normalized the data, we can use the Wilcoxon rank-sum test to check whether all of the features in the train and test sets seem to have come from the same superset. If so, they are similar as far as their feature averages are concerned, and ostensibly, they are representative of the superset. A higher alpha indicates more similarity between sets on that feature.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# # Uncomment to replit and rewrite.
# full_set_len = nrow(kaggle_train_set)
# test_set_len = as.integer(.3 * full_set_len)
# num_set = select(kaggle_train_set, where(is.double))
# seed_idx = sample(x = full_set_len, size = 15)
# seed_set = num_set[seed_idx, ]
# pool_set = num_set[-seed_idx, ]
# 
# test_set = maxDissim(
#   a = seed_set,
#   b = pool_set,
#   n = test_set_len,
#   obj =sumDiss
# )
# 
# write.csv(x = test_set, file = 'data/maxDissim_split_idx.csv')

# Read previous split.
test_set = read.csv(file = 'data/maxDissim_split_idx.csv')
test_set = test_set$x

y_cols = c('SalePrice')
id_cols = c('Id')
cont_feats_lst = colnames(select(kaggle_train_set, where(is.double)))
cont_feats_lst = c(cont_feats_lst, y_cols)
feats_p_val_lst = vector(mode = 'list', length = length(cont_feats_lst))
names(feats_p_val_lst) = cont_feats_lst

maxD_validate_split_lst = validate_split(
  train = kaggle_train_set[-test_set, ],
  test = kaggle_train_set[test_set, ],
  feats_lst = cont_feats_lst,
  y_cols = y_cols,
  feats_p_val_lst = feats_p_val_lst,
  alpha = 0,
  target_alpha = 0 
)
print("p-values of maxDissim:")
print(maxD_validate_split_lst$p_vals)
```

We can be pretty confident in the split on some of these features (e.g. p > .9), but we can also be pretty confident that the train and test sets imply very different supersets according to some of these features (e.g. p < .1). Many are ambiguous. I ran this many times and came up with similar results each time. (I'm p-hacking again, but it's for exploration, not for a hypothesis test that I need to rely on.)

What about adversarial Validation?

```{r echo=TRUE, message=FALSE, warning=FALSE}
# # Uncomment to revalidate.
# maxD_split_av = adversarial_validation(
#   train = select(kaggle_train_set[-test_set, ], -c('Id', 'SalePrice')),
#   test = select(kaggle_train_set[test_set, ], -c('Id', 'SalePrice'))
# )
# saveRDS(maxD_split_av, 'data/maxD_split_av.rds')

maxD_split_av = readRDS('data/maxD_split_av.rds')

summary(maxD_split_av$results$Accuracy)
```

XGBoost is able to somewhat detect the difference between the train and test sets.

Also, maxDissim only takes numeric features into account. Since 20 out of 80 features in this set are numeric, perhaps this split method will at least improve the representativity of the factors as well. Or, perhaps it will bias the split toward models that rely more heavily on the numeric variables.

#### Random Search with Wilcoxon Rank-Sum

I've found ways to create a stratified sample, but only using a single factor.

Having found no other built-in way to create a well-stratified/representative split across multiple features, I wrote a simple algorithm to ensure the random sample used for the split results in sets that are relatively representative. It simply takes a random sample and runs the same validation as above. If a single feature fails the Wilcoxon test according to the alpha we choose, resample and try again until finding a sample in which all features have p > alpha.

This algorithm is subject to the same problem as maxDissim of only considering numeric features. And, it typically runs much longer (depending on alpha), potentially infinitely. But, it does produce an ostensibly more representative split. [See the Github repo I created for it for further discussion.](https://github.com/KalebCoberly/train_test_split_R){target="_blank"}

One could use maxDissim within this algorithm, using maxDissim to create each sample rather than a simple random sample. But, compute would become even more prohibitive, maxDissim running orders of magnitude longer than taking a random sample. You could reduce compute by reducing alpha and/or using a low value for randomFrac in maxDissim to shrink the subset within which to search for the maximum dissimilar data point. I chose to simply use the split I found through my algorithm.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# ##### Uncomment this chunk to split again.
# # Otherwise, run next chunk to read in the results of last split. #####


# # Treating Kaggle train set as the full set,
# # split the final test set out,
# # and split the validation set out from that.
# # That way can validate a couple of models, even if using cross-validation.
# # Write to CSV files to keep same random split between uses.
#
#
# # Using data.frame, which maintains index in some operations and not in others,
# # so keeping Id column to be safe.
# y_cols = c('SalePrice')
# id_cols = c('Id')
# cont_feats_lst = colnames(select(wrangle_lst$df, where(is.double)))
# cont_feats_lst = c(cont_feats_lst, y_cols)
# alpha = 0.5
# target_alpha = 0.9
# 
# full_split_lst = train_test_split(
#   df = wrangle_lst$df,
#   y_cols = y_cols,
#   id_cols = id_cols,
#   feats_lst = cont_feats_lst,
#   alpha = alpha,
#   target_alpha = target_alpha
# )
# # > names(full_split_lst)
# # [1] "train_X" "train_y" "test_X"  "test_y"
# 
# val_split_lst = train_test_split(
#   df = merge(
#     full_split_lst$train_X,
#     full_split_lst$train_y,
#     by = id_cols,
#     all = TRUE
#   ),
#   y_cols = y_cols,
#   id_cols = id_cols,
#   feats_lst = cont_feats_lst,
#   alpha = alpha,
#   target_alpha = target_alpha
# )
# # > names(val_split_lst)
# # [1] "train_X" "train_y" "test_X"  "test_y"

# # Uncomment the following line to write new split to files.

# # # Write to CSV files to keep same random split between uses.
# # file_path = 'data/'
# # write_sets(set_lst = full_split_lst, prefix = 'full_', file_path = file_path)
# # write_sets(set_lst = val_split_lst, prefix = 'val_', file_path = file_path)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Read in saved splits.
# "_re_" because these are more rigorous splits than the ones I did a
# once-through EDA with initially while the splitter ran with higher criteria.
full_train_X = read.csv(file = 'data/full_re_train_X.csv')
full_train_y = read.csv(file = 'data/full_re_train_y.csv')
full_test_X = read.csv(file = 'data/full_re_test_X.csv')
full_test_y = read.csv(file = 'data/full_re_test_y.csv')
val_train_X = read.csv(file = 'data/val_re_train_X.csv')
val_train_y = read.csv(file = 'data/val_re_train_y.csv')
val_test_X = read.csv(file = 'data/val_re_test_X.csv')
val_test_y = read.csv(file = 'data/val_re_test_y.csv')

# Re-wrangle to set data types again after reading in fresh from CSV.
# Only need the types reset, but I nested that function in wrangle() to avoid
# cluttering my workspace with all the wrangle subroutines. Should have just
# deleted them.
full_train_X_wrangle_lst = wrangle(df = full_train_X)
full_train_X = full_train_X_wrangle_lst$df
full_test_X_wrangle_lst = wrangle(df = full_test_X)
full_test_X = full_test_X_wrangle_lst$df
full_train_y$SalePrice = as.numeric(full_train_y$SalePrice)
full_test_y$SalePrice = as.numeric(full_test_y$SalePrice)

val_train_X_wrangle_lst = wrangle(df = val_train_X)
val_train_X = val_train_X_wrangle_lst$df
val_test_X_wrangle_lst = wrangle(df = val_test_X)
val_test_X = val_test_X_wrangle_lst$df
val_train_y$SalePrice = as.numeric(val_train_y$SalePrice)
val_test_y$SalePrice = as.numeric(val_test_y$SalePrice)

rm(full_train_X_wrangle_lst, full_test_X_wrangle_lst, val_train_X_wrangle_lst,
   val_test_X_wrangle_lst)
```

#### Compare Representation

##### By Wilcoxon p-values

<a id="comp_by_wilcox"></a>

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Get p-values for each variable. (i.e. How representative is this split?)
# Greater p-value is better in this case, representing the risk of incorrectly
# rejecting the null hypothesis that the sets came from the same superset.

# Using data.frame, which maintains index in some operations and not in others,
# so keeping Id column to be safe.
y_cols = c('SalePrice')
id_cols = c('Id')
# Not excluding continuous variables that were excluded in split testing.
cont_feats_lst = colnames(select(kaggle_train_set, where(is.double)))
cont_feats_lst = c(cont_feats_lst, y_cols)
alpha = 0
target_alpha = 0
feats_p_val_lst = vector(mode = 'list', length = length(cont_feats_lst))
names(feats_p_val_lst) = cont_feats_lst

full_validate_split_lst = NULL
full_validate_split_lst = validate_split(
  train = merge(x = full_train_y, y = full_train_X, by = c('Id')),
  test = merge(x = full_test_y, y = full_test_X, by = c('Id')),
  feats_lst = cont_feats_lst,
  y_cols = y_cols,
  feats_p_val_lst = feats_p_val_lst,
  alpha = alpha,
  target_alpha = target_alpha 
)

val_validate_split_lst = NULL
val_validate_split_lst = validate_split(
  train = merge(x = val_train_y, y = val_train_X, by = c('Id')),
  test = merge(x = val_test_y, y = val_test_X, by = c('Id')),
  feats_lst = cont_feats_lst,
  y_cols = y_cols,
  feats_p_val_lst = feats_p_val_lst,
  alpha = alpha,
  target_alpha = target_alpha 
)

split_pvals_df = t(
  bind_rows(
    list(
      Kag_validate_split_lst$p_vals,
      maxD_validate_split_lst$p_vals,
      full_validate_split_lst$p_vals,
      val_validate_split_lst$p_vals
    )
  )
)
split_pvals_df = data.frame(split_pvals_df)
split_pvals_df = rename(
  split_pvals_df,
  c(
    Kag_split_pvals = X1,
    maxD_split_pvals = X2,
    full_split_pvals = X3,
    val_split_pvals = X4
  )
)
split_pvals_df$product =
  split_pvals_df$Kag_split_pvals *
  split_pvals_df$full_split_pvals *
  split_pvals_df$val_split_pvals
split_pvals_df
summary(split_pvals_df)

split_pvals_df_melted = melt(split_pvals_df)
fenced_jbv(
  data = split_pvals_df_melted,
  x = 'variable',
  y = 'value',
  jit_h = 0
)
```

More of the features are more closely aligned across train and test sets with this split than with the `maxDissim` split, according to the Wilcoxon rank sum test.

Because I split twice after the Kaggle split (full split and validation split), the probability that the final validation split represents the original superset is better measured by the product of the p-values of all splits (product column).

Having run this random search algorithm several times with different alpha values, I used each feature's average p-values over all samples (unsuccessful samples included) to determine what was a reasonable minimum alpha to set and avoid overly long run time. Average p-values ranged from .45 to .6, so I set alpha to .5 for all features, meaning there's at least a 50% chance of incorrectly rejecting the null hypothesis that the train and test sets represent the same superset for each feature.

I did, however, set a higher alpha (.9) for the target feature, SalePrice. It's imperative that this feature is representative.

##### By Adversarial Validation

<a id="comp_by_AV"></a>

How do these splits stack up with adversarial validation?

```{r echo=TRUE, message=FALSE, warning=FALSE}
# # Uncomment to revalidate.
# full_split_av = adversarial_validation(
#   train = select(full_train_X, -c('Id')),
#   test = select(full_test_X, -c('Id'))
# )
# saveRDS(full_split_av, 'data/full_split_av.rds')

full_split_av = readRDS('data/full_split_av.rds')

summary(full_split_av$results$Accuracy)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# # Uncomment to revalidate.
# val_split_av = adversarial_validation(
#   train = select(val_train_X, -c('Id')),
#   test = select(val_test_X, -c('Id'))
# )
# saveRDS(val_split_av, 'data/val_split_av.rds')

val_split_av = readRDS('data/val_split_av.rds')

summary(val_split_av$results$Accuracy)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
AV_accuracy_df = rename(
  melt(
    data.frame(
      list(
        'Kaggle' = kag_split_av$results$Accuracy,
        'maxDissim' = maxD_split_av$results$Accuracy,
        'test_split' = full_split_av$results$Accuracy,
        'validation_split' = val_split_av$results$Accuracy
      )
    )
  ),
  c(Split_Method = variable, Accuracy = value)
)

###
### FIXME
# Not sure why my fenced_jbv is wonky without jit_h = 0.
# source("tools/eda.R")
###
fenced_jbv(
  data = AV_accuracy_df,
  x = 'Split_Method',
  y = 'Accuracy',
  jit_h = 0
)

# ggplot(AV_accuracy_df, aes(x = Split_Method, y = Accuracy)) +
#   geom_jitter(
#     position = position_jitter(h = 0),
#     alpha = 0.25,
#     color = 'blue'
#     ) +
#   geom_boxplot(notch = T, varwidth = T, alpha = 0) +
#   geom_violin(alpha = 0)

# print("While the data are non-normal, Cohen's d is illuminating. Here's Cohen's d between the maxDissim adversarial validation results and those of the split using my method:")
# print(
#   cohen.d(
#     AV_accuracy_df[AV_accuracy_df$Split_Method == 'test_split', 'Accuracy'],
#     AV_accuracy_df[AV_accuracy_df$Split_Method == 'maxDissim', 'Accuracy']
#   )
# )
```

It looks like my high-compute method produced a significantly less biased split than caret's `maxDissim` did. My final split (the validation split) appears to be significantly more biased than the test split from which it came. But, it is about as biased as the maxDissim split.

None of these are anywhere near as representative as the Kaggle split. That said, Kaggle used a 50-50 split, whereas I used a 70-30 split, which may account somewhat for the difference in accuracy variance and mean. Kaggle's split method remains a black box to me, but I imagine they incorporated multifactor stratification in order to do so well with a classification tree, whereas the two methods I used only considered continuous variables.

# EDA and Engineering

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = merge(
  x = val_train_X,
  y = val_train_y,
  by = 'Id',
  all = TRUE
)
```

There are 715 total observations in the validation training set, across 81 features (target feature "Saleprice" and id column "Id" included). 46 features are factors, 24 of which are ordered. 14 are integers. 20 are doubles, including "SalePrice". ("Id" is integers cast as a character type.)

## Correlations

The full correlation grid is too large for most screens, but there are only a handful of noteworthy correlations which I'll include with further analysis of each feature.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# ggcorr(
#   select(val_train_Xy, where(is.numeric)),
#   label = T,
#   label_round = 2,
#   label_size = 3
# )
```

## Normalizing Continuous Variables

I wrote a simple algorithm to try various transformations on each continuous feature and use the [Shapiro-Wilk Normality Test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test){target="_blank"} to choose the best transformation. I then visualize each feature and decide if it even makes sense to attempt normalization.

I should have made logarithmic transformations be that of x+1, but instead I excluded 0s, which only partially handled the issue. 1s still convert to 0s in that case. The result was that variables with a substantial number of 1s did not find logs very useful for normalization.

I also did not include more dynamic transformations like Box-Cox. The script could be modified to include them.

For variables in which a 0 indicates a missing feature, I normalized only the non-zero set. The idea was that it might aid regression when the variable is put into interaction with its missingness.

```{r echo=TRUE, message=FALSE, warning=FALSE}
funcs_lst = list(
    'no_func' = function (x) { x },
    'sqrt' = sqrt,
    'cbrt' = function(x) { x^(1/3) },
    'square' = function(x) { x^2 },
###
### FIXME
# Make log transformations of x+1.
###
    'log' = log,
    'log2' = log2,
    'log10' = log10,
    '1/x' = function (x) { 1/x },
    '2^(1/x)' = function (x) { 2^(1/x) }
    # Box Cox: write function that calls MASS::boxcox and include lambda in results/function returned.
    # Yeo-Johnson
    # Winsorize here?
    # Rank
    # Rank-Gauss
  )
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

print("Best normalizing transformations:")
for (feat in names(best_normalizers)) {
  func_name = best_normalizers[[feat]]$best_func$name
  print(
    paste(
      feat, ":", func_name,
      ", p-value:", best_normalizers[[feat]]$results[[func_name]]$p.value
    )
  )
}
```

## SalePrice

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'SalePrice'
summary(val_train_Xy[[x]])

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 5000,
  t_binw = 1/50
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('log(SalePrice)' = log(SalePrice))

x = 'log(SalePrice)'

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1/100,
  t_binw = 1/100
)
```

A natural log best normalizes the sale price distribution. However, because it isn't a log10 transformation, it won't precisely scale the prediction errors proportionally to the sale price. I could simply use the log10 instead, which does a fair job at normalizing as well, but I want to stick with the "best" transformation to make the best model. So, when I run ML, I will write a custom summary function to train with which simply divides the error by the price (the log(SalePrice)) before calculating the RMSE.

### Winsorize

Here I'm looking for the best Winsorization quantile values of the best transformation -- the best according to Shapiro-Wilk p-values. I could have programmatically explored the space and returned the best result. But, I want to visualize it and explore the process itself. In future projects, I might choose to further automate this.

It should be noted that Winsorization of the target variable should only be used for training, not for testing. A log transformation can be reversed as a vectorized operation, but Winsorization can't. Winsorization should improve the accuracy of the model, but would be cheating on the test.

```{r echo=TRUE, message=FALSE, warning=FALSE}
qqnorm(y = val_train_Xy$SalePrice, ylab = 'SalePrice')
qqline(y = val_train_Xy$SalePrice, ylab = 'SalePrice')

qqnorm(y = val_train_Xy$`log(SalePrice)`, ylab = 'log(SalePrice)')
qqline(y = val_train_Xy$`log(SalePrice)`, ylab = 'log(SalePrice)')

Win_log_x = Winsorize(
  x = val_train_Xy[['log(SalePrice)']],
  probs = c(0.005, 0.995)
)

qqnorm(y = Win_log_x, ylab = 'Win_log_x')
qqline(y = Win_log_x, ylab = 'Win_log_x')

Win_raw_x = Winsorize(
  x = val_train_Xy[['SalePrice']],
  probs = c(0, 0.95)
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy$SalePrice))
print(shapiro.test(x = val_train_Xy$`log(SalePrice)`))
print(shapiro.test(x = Win_log_x))
print(shapiro.test(x = Win_raw_x))
```

A small Winsorization of the log best normalizes the variable (W = 0.99062). It doesn't pass the test for normality (p < 0.01), but it is still better prepared for a linear regression.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(SalePrice)' = Winsorize(
      SalePrice,
      probs = c(0, 0.95),
      na.rm = T
    )
  ) %>%
  mutate(
    'Win(log(SalePrice))' = Winsorize(
      log(SalePrice),
      probs = c(0.005, 0.995),
      na.rm = T
    )
  )
```

### Correlations

Here are the correlations between SalePrice and the rest of the variables, compared to those of the transformed variables. Transforming SalePrice resulted in minor increases of correlations to many other continuous features and some minor reductions of correlations, an overall minor and insignificant improvement. But, this is the first of the variables to be transformed.

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x = 'Win(log(SalePrice))'
x_lst = c('SalePrice', 'log(SalePrice)', 'Win(log(SalePrice))',
          'Win(SalePrice)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

### Hard Code

I'll need to hard code the top and bottom limits for the engineering script to apply to the test set without leakage. And, I'll need to drop Win(SalePrice).

I want to keep the transformed and non-Winsorized version though. In the case of the target variable, I'll train on the Winsorized variable and test on the transformed because I can reverse the transformation. In the case of predictor variables, the Winsorized version will be good for basic linear regression without interactions, but not necessarily for KNN and RF which may be able to use the outliers for clustering and grouping.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(log(SalePrice))'

min_val = min(val_train_Xy[[x]])
max_val = max(val_train_Xy[[x]])
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(log(SalePrice))' = Winsorize(
      .data[['log(SalePrice)']],
      minval = min_val,
      maxval = max_val
    )
  ) %>%
  select(-c('Win(SalePrice)'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1/50)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

### SalePrice as Factor

To aid visualization, I'll create a SalePrice factor with extremes and quartiles as levels.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'SalePrice.fact' = cut(
      x = SalePrice,
      breaks = quantile(x = SalePrice),
      include.lowest = T,
      ordered_result = T
    )
  )

summary(val_train_Xy$SalePrice.fact)
```

## MSSubClass (Dwelling Type)

[Back to top.](#top)

I'll use log(SalePrice) to visualize against factors, rather than the Winsorized version.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'MSSubClass'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

The most common class is 1-story newer than 1945 (267), followed by 2-story newer than 1945 (149) and 1.5-story finished all ages (67). The priciest class is 2-story newer than 1945, though some classes are so uncommon that it's hard to say this completely confidently.

This feature is a mix of information mostly covered by HouseStyle, YearBuilt, and square footage. It might be worth dropping it to avoid overweighting this info, avoid spurious fit, and skip the compute cost of 16 one-hot features. Alternatively, decomposition with PCA might help pull out the unique information regarding PUD housing.

## MSZoning

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'MSZoning'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Mostly residential low density (574), some medium density (100), fewer floating village residential (flexible zoning, 31). Predictive power may be limited due to lack of diversity. That said, low-density residential and floating village clearly tend to sell for more than medium-density. Consider only one-hot encoding RL, RM, and FV?

 
## LotFrontage

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'LotFrontage'

summary(val_train_Xy[[x]])

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 5,
  t_binw = 1/50
)
```

A log10 scale centers it better (133 missing values excluded).

Note the extreme spike (~65 observations) in left of mode (70-75 SF) at 60 SF. It doesn't seem to be associated with any particular neighborhood or lot configuration or anything, but probably just a common way to cut lots.

The feature could benefit from top/bottom coding.

133 NAs. Counterintuitively, a lower proportion of missing LotFrontages are inside lots (55/121 in the NA subset vs. 511/715 in the training set [these numbers are from a previous split and not accurate for the current data set]), whereas many lots that by definition have frontage (44 corner lots, FR2, and FR3) are missing frontage values. Could use LotArea, LotShape, LotConfig, and (?) (all of which aren't missing values) to multivariate impute.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'log10(LotFrontage)'
val_train_Xy = val_train_Xy %>%
  mutate(
    'log10(LotFrontage)' = ifelse(
    LotFrontage == 0,
    0,
    log10(LotFrontage)
    )
  )

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1/50,
  t_binw = 1/50
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
qqnorm(y = val_train_Xy$LotFrontage, ylab = 'LotFrontage')
qqline(y = val_train_Xy$LotFrontage, ylab = 'LotFrontage')

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

Win_log10_x = Winsorize(
  x = val_train_Xy[[x]],
  probs = c(0.05, 0.99),
  na.rm = T
)

qqnorm(y = Win_log10_x, ylab = 'Win_log10_x')
qqline(y = Win_log10_x, ylab = 'Win_log10_x')

Win_raw_x = Winsorize(
  x = val_train_Xy$LotFrontage,
  probs = c(0.05, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win(LotFrontage)')
qqline(y = Win_raw_x, ylab = 'Win(LotFrontage)')

print(shapiro.test(x = val_train_Xy$LotFrontage))
print(shapiro.test(x = val_train_Xy[[x]]))
print(shapiro.test(x = Win_log10_x))
print(shapiro.test(x = Win_raw_x))
```

It looks like just Winsorizing the raw variable may be the way to go here.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(LotFrontage)' = Winsorize(
      LotFrontage,
      probs = c(0.05, 0.95),
      na.rm = T
      )
    ) %>%
  mutate(
    'Win(log10(LotFrontage))' = Winsorize(
      log10(LotFrontage),
      probs = c(0.05, 0.99),
      na.rm = T
      )
    )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(LotFrontage)'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('LotFrontage', 'log10(LotFrontage)', 'Win(log10(LotFrontage))', 'Win(LotFrontage)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(LotFrontage)'

min_val = min(val_train_Xy[!is.na(val_train_Xy[[x]]), x])
max_val = max(val_train_Xy[!is.na(val_train_Xy[[x]]), x])
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(LotFrontage)' = Winsorize(
      LotFrontage,
      minval = min_val,
      maxval = max_val
    )
  )

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

### By Factors

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('MSSubClass', 'MSZoning', 'LotShape', 'LotConfig', 'Neighborhood',
          'BldgType', 'HouseStyle')
for (y in y_lst) {
  plt = fenced_jbv(
    data = val_train_Xy, x = y, y = 'log10(LotFrontage)') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(plt)
}
```

Overall, the clusters of lots at 60' and 80' is quite apparent.

Unsurprisingly, low-density residential tends to have more lot frontage than medium-density residential. Slightly irregular lots might tend to have more frontage than regular, but we can't say that with much confidence. Corner lots have more frontage than inside lots. There's quite a bit of variation between neighborhoods.

Looking at MSSubClass, older homes tend to have less lot frontage than their equivalent house styles after 1945, unless they are PUD homes, which tend to have much less frontage than the rest of the classes. This connection is not visible in YearBuilt, which has no correlation to LotFrontage.

There's also an interesting gap between 50' and 60' that only homes older than 1945 tend to fill, except for PUD homes.

```{r echo=TRUE, message=FALSE, warning=FALSE}
fenced_jbv(
  data = val_train_Xy,
  x = 'MSSubClass',
  y = 'log10(LotFrontage)',
  jit_col = 'SalePrice.fact',
  leg_lbl = 'SalePrice',
  jit_alpha = 0.5,
  box_color = 'red'
)

ggplot(val_train_Xy, aes(x = `log10(LotFrontage)`, y = `log(SalePrice)`)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm') +
  facet_wrap(vars(MSSubClass), ncol = 5)
```

## LotArea 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'LotArea'
summary(val_train_Xy[[x]])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 200,
  t_binw = 1/50
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x_trans = 'log10(LotArea)'
val_train_Xy = val_train_Xy %>%
  mutate('log10(LotArea)' = log10(LotArea))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x_trans])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x_trans,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1/50,
  t_binw = 1/500
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x_trans = 'log10(log10(LotArea))'
val_train_Xy = val_train_Xy %>%
  mutate('log10(log10(LotArea))' = log10(log10(LotArea)))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x_trans])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x_trans,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1/500,
  t_binw = 1/750
)
```

I doubt it's worth doing the third log10 transformation now that the median and mean are so close. It still needs top- and bottom-coding anyway.

Even the second transformation might lead to overfit, but I'll roll with it.

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
qqnorm(y = val_train_Xy$LotArea, ylab = 'LotArea')
qqline(y = val_train_Xy$LotArea, ylab = 'LotArea')

qqnorm(y = val_train_Xy$`log10(LotArea)`, ylab = 'log10(LotArea)')
qqline(y = val_train_Xy$`log10(LotArea)`, ylab = 'log10(LotArea)')

qqnorm(
  y = val_train_Xy$`log10(log10(LotArea))`,
  ylab = 'log10(log10(LotArea))'
)
qqline(
  y = val_train_Xy$`log10(log10(LotArea))`,
  ylab = 'log10(log10(LotArea))'
)

Win_log10log10_x = Winsorize(
  x = val_train_Xy$`log10(log10(LotArea))`,
  probs = c(0.05, 0.99),
  na.rm = T
)

qqnorm(y = Win_log10log10_x, ylab = 'Win(log10(log10(LotArea)))')
qqline(y = Win_log10log10_x, ylab = 'Win(log10(log10(LotArea)))')

Win_log10_x = Winsorize(
  x = val_train_Xy$`log10(LotArea)`,
  probs = c(0.05, 0.99),
  na.rm = T
)

qqnorm(y = Win_log10_x, ylab = 'Win(log10(LotArea))')
qqline(y = Win_log10_x, ylab = 'Win(log10(LotArea))')

Win_raw_x = Winsorize(
  x = val_train_Xy$LotArea,
  probs = c(0.01, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'LotArea')
qqline(y = Win_raw_x, ylab = 'LotArea')

print(shapiro.test(x = val_train_Xy$LotArea))
print(shapiro.test(x = val_train_Xy$`log10(LotArea)`))
print(shapiro.test(x = val_train_Xy$`log10(log10(LotArea))`))
print(shapiro.test(x = Win_log10log10_x))
print(shapiro.test(x = Win_log10_x))
print(shapiro.test(x = Win_raw_x))
```

It looks like simply Winsorizing the base variable might be best.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(LotArea)' = Winsorize(
      LotArea,
      probs = c(0.01, 0.95),
      na.rm = T
      )
    ) %>%
  mutate(
    'Win(log10(LotArea))' = Winsorize(
      log10(LotArea),
      probs = c(0.05, 0.99),
      na.rm = T
      )
    )
```

### Correlations

Transforming LotArea with log10 resulted in bigger swings in r in both directions, but no real change in aggregate. The additional log10 transformation produced minor changes in correlation compared to the initial transformation, mostly toward less correlation.

Unsurprisingly, transformed LotArea is much more correlated to transformed LotFrontage than their untransformed counterparts (r went from 0.51 to 0.73). Also, transformed LotArea and transformed SalePrice correlate a little better than their untransformed counterparts, but are still weakly correlated (r increased from 0.30 to 0.37).

It also became noticeably more correlated to square footage of the first floor, bedrooms / total rooms above ground, and garage area/cars. Like previous transformations, some correlations lessened with this transformation, but none so much that the correlation dropped a bracket, e.g. from weak to insignificant.

The distribution of correlations dropped with the second transformation. I'll only use the first transformation in the engineering script.

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('LotArea', 'log10(LotArea)', 'log10(log10(LotArea))', 'Win(log10(LotArea))', 'Win(LotArea)')

x = 'Win(LotArea)'

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(df)

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(LotArea)'

min_val = min(val_train_Xy[!is.na(val_train_Xy[[x]]), x])
max_val = max(val_train_Xy[!is.na(val_train_Xy[[x]]), x])
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(LotArea)' = Winsorize(
      LotArea,
      minval = min_val,
      maxval = max_val
    )
  ) %>%
  select(-c('log10(LotArea)', 'Win(log10(LotArea))'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 100)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('log(SalePrice)', 'Win(LotFrontage)', 'X1stFlrSF',
          'GrLivArea','TotRmsAbvGrd', 'GarageArea')
x = 'Win(LotArea)'
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

### By Factors

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('MSSubClass', 'MSZoning', 'LotShape', 'LotConfig', 'Neighborhood',
          'BldgType', 'HouseStyle')
for (y in y_lst) {
  plt = fenced_jbv(
    data = val_train_Xy,
    x = y,
    y = 'log10(log10(LotArea))',
    jit_h = 0 # Again R randomly decides to go wonk unless I enter the default.
  ) +
    theme(axis.text.x = element_text(angle = 45, hjust=1))
  print(plt)
}
```

There are similar patterns as in LotFrontage, with a more marked upward trend against LotShape, and with less difference between lot configurations. There's also an interesting pocket of two-story houses with low lot area; it's not worth plotting, but you can see which neighborhoods these are.

## Street

[Back to top.](#top)

Really lopsided to paved (3 gravel, 712 paved). Drop this feature.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$Street)
val_train_Xy = select(val_train_Xy, -c('Street'))
```

## Alley

[Back to top.](#top)

Vast majority have none, drop it.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$Alley)
val_train_Xy = select(val_train_Xy, -c('Alley'))
```

## LotShape

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'LotShape'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Irregularly shaped lots tend to sell for more. Good candidate for binarization if doing a basic linear regression with no interactions; one-hot encode 'Reg' and drop the rest of the levels.

## LandContour

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$LandContour)
val_train_Xy = select(val_train_Xy, -c('LandContour'))
```

## Utilities

[Back to top.](#top)

All all-public. Definitely drop.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$Utilities)
val_train_Xy = select(val_train_Xy, -c('Utilities'))
```

## LotConfig

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'LotConfig'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

This set is mostly inside lots (513) but plenty of corner lots (124). I've always thought corner lots are prized, but it doesn't seem to significantly add to price compared to an inside lot.

Cul de sacs are the priciest. This is somewhat a proxy for neighborhood (and maybe other features) as it is true across most neighborhoods except those that are already pricey (where cul de sacs are relatively more common) or least pricey (where cul de sacs don't typically exist).

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  data = val_train_Xy,
  # data = filter(
  #   val_train_Xy,
  #   LotConfig %in% c('Inside', 'Corner', 'CulDSac')
  # ),
  mapping = aes(
    x = Neighborhood,
    y = `log(SalePrice)`,
  )
) +
  geom_jitter(
    alpha = .4,
    mapping = aes(color = LotConfig, shape = LotConfig)
  ) +
  geom_boxplot(notch = T, varwidth = T, alpha = 0) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(
  data = val_train_Xy,
  mapping = aes(
    x = Neighborhood,
    y = `log(SalePrice)`,
  )
) +
  geom_jitter(
    alpha = .3,
    mapping = aes(color = LotShape, shape = LotShape)
  ) +
  geom_boxplot(notch = T, varwidth = T, alpha = 0) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(
  data = val_train_Xy,
  mapping = aes(
    x = LotConfig,
    y = `log(SalePrice)`,
  )
) +
  geom_jitter(
    alpha = .4,
    mapping = aes(color = LotShape, shape = LotShape)
  ) +
  geom_boxplot(notch = T, varwidth = T, alpha = 0)
```

<a id="lotshapeLotconfig"></a>

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  data = val_train_Xy,
  mapping = aes(
    x = LotShape,
    y = `log(SalePrice)`,
  )
) +
  geom_jitter(
    alpha = .4,
    mapping = aes(color = LotConfig, shape = LotConfig)
  ) +
  geom_boxplot(notch = T, varwidth = T, alpha = 0)
```

There appears to be some overlap with lot shape and lot configuration.

## LandSlope

[Back to top.](#top)

Vast majority are gentle drop it.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$LandSlope)
val_train_Xy = select(val_train_Xy, -c('LandSlope'))
```

## Neighborhood

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Neighborhood'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)

y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    keysize = 2
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

North Ames (Names) and CollgCr have the most residential homes (107 and 76). but they're also zoned low-density. A handful of neighborhoods have almost no houses in this set.

The priciest neighborhoods are StoneBr, NridgeHt, and NoRidge. The least pricey (OldTown, MeadowV, and IDOTRR) are also the most dense and commercial. There are significant differences in prices between many neighborhoods, and not just between the cheapest and priciest.

<a id="neighbZoning"></a>

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  data = val_train_Xy,
  aes(x = Neighborhood, fill = MSZoning)
) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Condition1, Condition2

[Back to top.](#top)

The vast majority are normal. Drop Condition2. But, for Condition1, there appear to be significant differences in SalePrice between Norm and Feedr, and maybe between Norm and Artery but there are too few Artery observations. It might be worth one-hot-encoding those categories.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Condition1'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

You might consider binarizing, lumping 'Feedr' and 'Artery' and maybe 'RRAe' together and lumping the rest with 'Norm'. I'll try it out during feature selection with caret in the ML phase.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$Condition2)
val_train_Xy = select(val_train_Xy, -c('Condition2'))
```

## BldgType

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BldgType'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Majority single-family (603), 24 NAs. It seems like an inherently important feature, despite the lopsidedness of the distribution. Probably safe to impute to mode (1Family), but other features might inform, such as MSSubClass, MSZoning, Neighborhood, HouseStyle, and building materials; multivariate impute might be in order, if keeping the feature in the first place.

Duplexes and two-family are significantly cheaper than single-family and townhouses, if accepting the low number in the sample. Candidate for binarization, but may lose interactions.

## HouseStyle

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'HouseStyle'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Mostly one-story (358), but many two-story (220). Several significant price differences across groups. Maybe worth keeping, but also kind of noisy with the finished/unfinished business only applying to half-stories, and number of stories and finished status are encoded in other features.

## OverallQual

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'OverallQual'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

There's a very strong relationship to SalePrice.

### Normalize

It's a pretty normal distribution, slightly left-skewed. Mode (2199 5s) left of median/mean (180 6s), few 1s, 2s, and 3s. It might be worth casting as an integer and transforming to normalize and possibly improve the correlation.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'OverallQual_int'

val_train_Xy = val_train_Xy %>%
  mutate(OverallQual_int = as.integer(OverallQual))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

None of the transformations improved its distribution.

### Correlations

Here are the correlations between OverallQual_int and the rest of the variables.

This feature has several moderate correlations to features having to do with size and age. It also has a strong correlation to transformed SalePrice (0.82).

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c(x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('log(SalePrice)', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF',
          'GrLivArea','FullBath', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageCars',
          'GarageArea')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

### By Factors

Again, there's a lot of interaction with MSSubClass and Neighborhood. Quality also decreases with zoning density. Two-story houses are generally rated higher than one-story houses. Vinyl gets rated highest among exteriors. Simply having masonry improves quality rating. Poured concrete foundations on average receive 7s, whereas cinder block and brick/tile receive 5s on average. The same is true for attached and built-in garages compared to detached and no garages. It almost looks like it's better to have no fence than to have one with minor privacy. Court officer deeds/estates are rated more poorly than warranty deeds and new sales. Abnormal sales are rated lower than normal sales.

Overall quality is doing a lot of the work for other features toward predicting price.

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('MSSubClass', 'MSZoning', 'Neighborhood', 'BldgType', 'HouseStyle',
          'RoofMatl', 'RoofStyle', 'Exterior1st', 'MasVnrType', 'Foundation',
          'Heating', 'Electrical', 'Functional', 'GarageType', 'GarageFinish',
          'Fence', 'MiscFeature', 'SaleType', 'SaleCondition')
for (y in y_lst) {
  plt = fenced_jbv(data = val_train_Xy, x = y, y = x) +
    theme(axis.text.x = element_text(angle = 45, hjust=1))
  print(plt)
}
```

## OverallCond

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'OverallCond'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

OverallCond is like OverallQual, but with a much more pronounced mode (397 5s) left of median/mean (124 6s), probably due to wear and tear being more universal than quality construction.

The correlation to SalePrice is weaker, and 5s oddly seem to sell for more on average than higher-rated houses.

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'OverallCond_int'

val_train_Xy = val_train_Xy %>%
  mutate(OverallCond_int = as.integer(OverallCond))

# Recalculate best normalizers. Might as well do them all, see if previous
# transformations benefit from further transformation, while we're at it.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```


### Correlations

This feature is only weakly correlated to a couple of age features. It has no linear correlation to SalePrice.

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c(x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('log(SalePrice)', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

The bump in price among houses of average condition seems to have to do with a cluster of houses made in the late '90s and early 2000s.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  data = val_train_Xy,
  mapping = aes(
    x = OverallCond_int,
    y = .data[['log(SalePrice)']],
    color = YearBuilt
  )
) +
  geom_jitter(alpha = 0.5) +
  geom_smooth() #+
  # facet_wrap(vars(BldgType))
```

### By Factors

OldTown seems to be in relatively good shape, while Edwards has proportionately more houses in need of work and reconditioning. You can easily spot clustered 5s in the neighborhoods that likely grew up in the building boom of the 2000s.

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('MSSubClass', 'MSZoning', 'Neighborhood', 'Condition1', 'BldgType',
          'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'MasVnrType',
          'BsmtExposure', 'Foundation', 'Heating', 'Electrical', 'Functional',
          'GarageType', 'GarageFinish', 'Fence', 'MiscFeature', 'SaleType',
          'SaleCondition')
for (y in y_lst) {
  plt = fenced_jbv(data = val_train_Xy, x = y, y = x) +
    theme(axis.text.x = element_text(angle = 45, hjust=1))
  print(plt)
}
```

Houses with metal and wood exteriors are typically in better condition than those with vinyl despite houses with vinyl siding typically being rated as higher quality and newer. Likewise, houses with cinder block foundations seem to fare better over time than those with poured concrete despite quality ratings, according to this data set, as do houses with detached garages compared to those with attached and built-in garages. Is this because older houses and lower-quality houses that are still standing are the ones that have received better maintenance?

```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 = ggplot(
  data = val_train_Xy,
  mapping = aes(
    y = OverallCond_int,
    x = Exterior1st,
    color = YearBuilt
  )
) +
  geom_jitter() +
  geom_boxplot(alpha = 0) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

p2 = ggplot(
  data = val_train_Xy,
  mapping = aes(
    y = OverallCond_int,
    x = Foundation,
    color = YearBuilt
  )
) +
  geom_jitter() +
  geom_boxplot(alpha = 0) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

p3 = ggplot(
  data = val_train_Xy,
  mapping = aes(
    y = OverallCond_int,
    x = GarageType,
    color = YearBuilt
  )
) +
  geom_jitter() +
  geom_boxplot(alpha = 0) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

grid.arrange(p1, p2, p3, ncol = 2)
```

## YearBuilt

[Back to top.](#top)

No transformations normalized the distribution. You can see the construction boom between WWII and the '80s (with a dip in mid '70s stagflation), followed by the relative explosion starting in the late '90s and dropping with the housing crisis in the 2000s.

Consider grouping around modes into a factor and dummy coding to accommodate some ML algorithms that wouldn't handle a polymodal distribution well.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'YearBuilt'
summary(val_train_Xy[x])

# sum_and_trans_cont(
#   data = val_train_Xy,
#   x = x,
#   func = best_normalizers[[x]]$best_func$func,
#   func_name = best_normalizers[[x]]$best_func$name,
#   x_binw = 1,
#   t_binw = 1
# )

ggplot(val_train_Xy, aes(x = YearBuilt)) +
  geom_histogram(binwidth = 1)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'YearBuilt'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c(x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = colnames(select(val_train_Xy, where(is.numeric)))
for (y in y_lst) {
  for (x in x_lst) {
    plt = ggplot(
      select(val_train_Xy, all_of(c(x, y))),
      aes(x = .data[[x]], y = .data[[y]])
    ) +
      geom_jitter() +
      geom_smooth() +
      labs(x = x, y = y) +
      scale_x_continuous(breaks = seq(1880, 2010, 5)) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
    print(plt)
  }
}
```

### By Factors

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = colnames(select(val_train_Xy, where(is.factor)))
for (y in y_lst) {
  plt = fenced_jbv(data = val_train_Xy, x = y, y = x) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(plt)
}
```

### A Story of Ames

<a id="AmesStory"></a>

Before looking at remodel year, this is the story I gather about houses purchased in this period based on the above visualizations of YearBuilt. I could try to weave the viz in with the narrative, but I'm not going to.

Houses and garages have gotten bigger over the years, with more bathrooms and less enclosed porch space. (I'm curious to see whether the increasing rarity of enclosed porch space led to it being a prized (priced) feature, or if it simply isn't as valued now.) Newer homes tend to be valued more, and builders generally began to produce higher-quality houses.

Most of the houses built in the post-WWII boom were single-story, but that's also when 2 1/2 stories became unheard-of. Split/multi-level and duplexes began to come on the scene. 1 1/2 stories had their heyday. Some of the houses from then and earlier make up the group of two-family conversions. It would be interesting to check remodel years to see when those conversions tended to take place.

As the century turned, prices grew exponentially. Two-story houses and PUD housing became more prevalent, and townhouses appeared for the first time. Zoning became less dense as new suburbs sprang up. Lots became more irregular. Cul de sacs and parks started to sprinkle in as feeder streets networked out. A handful of houses began to appear near railroads.

Even as masonry became a growing bling factor driving up overall quality, the age of plastic ushered in vinyl siding as the dominant exterior. The overall condition of houses from the turn of the century on is starkly average compared to older houses which are commonly in good shape. Checking against remodel years will likely explain some of this, but I'm curious to compare exterior conditions of different materials with regard to age.

The Lost Generation got brick and tile foundations. Boomers got cinder block and slabs. Gen x and Millenials got poured concrete. This enabled us to build into the sides of hills better and have more exposed basements with walkouts.

Most of the basementless houses were built during the post-WWII boom when ramblers were a common answer to the burgeoning dream of homeownership. Basement area continuously grew from then on, especially finished basement area as people began to live more underground. Kitchens and bedrooms began to be more common below ground.

Kitchens became more of a target for adding value with improved materials, construction, and appliances. Heating systems improved over the years as well. New electrical standards came into place in 1960.

When the '80s hit, the automobiled society was in full swing, and it was rare to see less than a two-car garage built anymore, let alone a house without a garage. Rather than a detached secondary building, garages became part of the main house itself. It was more often a finished space for more than just housing and working on cars, but without the need for high-quality construction. Unpaved driveways were now out of the question, though.

After 2000, virtually no new houses had fences, at least none that were bought during the years this data set covers.

### YearBuilt as Factor

I'll condense the housing booms around their modes in a factor that I can one-hot encode to accommodate some ML algorithms that wouldn't handle a polymodal distribution well. To maintain the model going forward, new periods will have to be identified and added.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Three periods: < 1945 < 1985ish < 2010
val_train_Xy = val_train_Xy %>%
  mutate(
    YearBuilt.fact = cut(
      x = YearBuilt,
      breaks = c(-Inf, 1944, 1986, Inf),
      ordered_result = T,
      labels = c('Before_1945', '1945_1986', 'After_1986')
    )
  )

p1 = ggplot(val_train_Xy, aes(x = YearBuilt)) +
  geom_histogram(binwidth = 1) +
  scale_x_continuous(breaks = seq(1880, 2010, 5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

p2 = ggplot(val_train_Xy, aes(x = YearBuilt.fact)) +
  geom_bar()

grid.arrange(p1, p2)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'YearBuilt.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)

y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## Age

[Back to top.](#top)

It might make sense to transform year built into the age of the house when bought. The years of sales is a small range, but it may add a touch more predictive information, especially for the newer houses.

### Normalize

It seems to have added a couple of outliers, but applying a square-root transformation better centers it and removes the outliers.

0s don't indicate a missing feature, so I want to include them in the search for the best normalizer.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('Age' = (YrSold - YearBuilt))

x = 'Age'

# Include 0s. Just recalculate rather than rewrite code.
num_feats = c(x)
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(-1)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1/10
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(Age)'
val_train_Xy = val_train_Xy %>%
  mutate('sqrt(Age)' = sqrt(Age))

# Include 0s. Just recalculate rather than rewrite code.
num_feats = c(x)
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(-1)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1/10,
  t_binw = 1/10
)

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('YearBuilt', 'Age', 'sqrt(Age)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('log(SalePrice)', 'Win(log(SalePrice))', 'GarageArea', 'GarageCars',
          'EnclosedPorch', 'OpenPorchSF', 'OverallQual_int', 'OverallCond_int')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

There's a small increase in linear correlation of Age to transformed SalePrice from YearBuilt (0.59 to 0.62). Correlations to some measures of size strengthened. The correlation to quality rose quite a bit (from 0.59 to 0.65) while the correlation to condition only rose slightly (0.36 to 0.37), remaining weak.

With YearBuilt, there's still a polynomial appearance to the plot against log(SalePrice), but that's smoothed out with sqrt(Age). YearRemodAdd may add clarity.

<a id="condAge"></a>

The concentration of average-condition houses among the youngest is still puzzling. It's as if a few years of aging tends to improve the condition of a house. Perhaps owners tend to add cosmetic value in the first years. Maybe assessors use the youngest houses as the anchor by which to assess all others. Many of these houses were unfinished, which explains some but not all of this.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  val_train_Xy,
  aes(
    x = .data[['sqrt(Age)']],
    y = OverallCond_int,
    shape = SaleCondition,
    color = SaleCondition
  )
) +
  geom_jitter()
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(val_train_Xy, -c('Age'))
```

## YearRemodAdd

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'YearRemodAdd'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

There was a curiously large number of remodels in 1950, about 80 remodels that year compared to the peak of about 50 in the 2000s. These were all built before 1950, and no house has an earlier remodel year than 1950, and some houses built before 1950 have remodel years later than 1950.

I'll treat houses with a 1950 remodel as if they have not had a remodel; they may have had an earlier remodel, but I'm guessing that those older remodels are of little added value at this point. Ames assessors may have had reason to bottom-code, but I'll set remodel year to built year in years prior to 1950 for now and see if it helps or hinders analysis and prediction.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(YearRemodAdd.uncode = ifelse(YearRemodAdd == 1950, YearBuilt, YearRemodAdd))
x = 'YearRemodAdd.uncode'
# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
# sum_and_trans_cont(
#   data = val_train_Xy,
#   x = x,
#   func = best_normalizers[[x]]$best_func$func,
#   func_name = best_normalizers[[x]]$best_func$name,
#   x_binw = 1,
#   t_binw = 1
# )
ggplot(val_train_Xy, aes(x = .data[[x]])) +
  geom_histogram(binwidth = 1)
```

### Correlations

This only added outliers and slightly weakened the correlation to SalePrice.

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('YearBuilt', 'YearRemodAdd', 'YearRemodAdd.uncode')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('log(SalePrice)')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

### By Factors

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = colnames(select(val_train_Xy, where(is.factor)))
for (y in y_lst) {
  plt = fenced_jbv(data = val_train_Xy, x = y, y = x) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(plt)
}
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(val_train_Xy, -c('YearRemodAdd.uncode'))
```

Removing the records in which there is no remodel (i.e. YearRemodAdd == YearBuilt or 1950) adds clarity. 368 rows were removed for no remodel, and remodels really started being recorded in increasing numbers starting in the '90s, maybe a little in the late '80s.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  val_train_Xy,
  aes(x = ifelse(YearRemodAdd == YearBuilt, NA, YearRemodAdd)
  )
) +
  geom_histogram(binwidth = 1)
```

### YearRemodAdd as Factor

Because there are so many houses without remodels, I'll split it into a factor, a level for each decade with the year 1950 conveniently lumped in with NAs.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'YearRemodAdd.fact'
val_train_Xy = val_train_Xy %>%
  mutate(
    YearRemodAdd.fact = factor(
      cut(
        ifelse(
          YearRemodAdd == YearBuilt | is.na(YearRemodAdd),
          1949,
          YearRemodAdd
        ),
        breaks = c(1949, 1950, 1960, 1970, 1980, 1990, 2000, 2010),
        labels = c('None', '50s', '60s', '70s', '80s', '90s', '00s')
      )
    )
  ) #%>%
  # select(-c('YearRemodAdd'))

y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## RoofStyle

[Back to top.](#top)

Most are Gable (563), and many are Hip (139). Flat, Gambrel, and Mansard are neglible.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'RoofStyle'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## RoofMatl

[Back to top.](#top)

Vast majority are composition shingle (702). Can drop this feature.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$RoofMatl)
val_train_Xy = select(val_train_Xy, -c('RoofMatl'))
```

## Exterior1st/2nd

[Back to top.](#top)

Most popular class is vinyl (255 and 249), but wood, metal, and others represent significant classes. No 'None' in 2nd, so all houses in Ames have at least two types of siding?

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Exterior1st'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Exterior2nd'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## MasVnrType

[Back to top.](#top)

Most have none (430), but plenty of brick (219) and stone (66). No cinderblock in the split.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'MasVnrType'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## MasVnrArea

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'MasVnrArea'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$MasVnrArea != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 10,
  t_binw = 0.25
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(MasVnrArea)'
val_train_Xy = val_train_Xy %>%
  mutate('cbrt(MasVnrArea)' = MasVnrArea^(1/3))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$MasVnrArea != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.25,
  t_binw = 0.25
)
```

### Winsorize

Because this variable's 0s indicate a missing feature, we're really concerned with Winsorizing non-zero values.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = val_train_Xy[val_train_Xy$MasVnrArea != 0, ]

qqnorm(y = df$MasVnrArea, ylab = 'MasVnrArea')
qqline(y = df$MasVnrArea, ylab = 'MasVnrArea')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

qqnorm(y = sqrt(sqrt(df$MasVnrArea)), ylab = 'sqrt(sqrt(MasVnrArea))')
qqline(y = sqrt(sqrt(df$MasVnrArea)), ylab = 'sqrt(sqrt(MasVnrArea))')

Win_sqrt_x = Winsorize(
  x = sqrt(sqrt(df$MasVnrArea)),
  probs = c(0.005, 0.995),
  na.rm = T
)

qqnorm(y = Win_sqrt_x, ylab = 'Win(sqrt(sqrt(MasVnrArea)))')
qqline(y = Win_sqrt_x, ylab = 'Win(sqrt(sqrt(MasVnrArea)))')

Win_cbrt_x = Winsorize(
  x = df$`cbrt(MasVnrArea)`,
  probs = c(0.005, 0.995),
  na.rm = T
)

qqnorm(y = Win_cbrt_x, ylab = 'Win(cbrt(MasVnrArea))')
qqline(y = Win_cbrt_x, ylab = 'Win(cbrt(MasVnrArea))')

Win_raw_x = Winsorize(
  x = df$MasVnrArea,
  probs = c(0.05, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win(MasVnrArea)')
qqline(y = Win_raw_x, ylab = 'Win(MasVnrArea)')

print(shapiro.test(df$MasVnrArea))
print(shapiro.test(df$'cbrt(MasVnrArea)'))
print(shapiro.test(sqrt(sqrt(df$MasVnrArea))))
print(shapiro.test(Win_sqrt_x))
print(shapiro.test(Win_cbrt_x))
print(shapiro.test(Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
min_val = min(Win_cbrt_x)
max_val = max(Win_cbrt_x)
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(cbrt(MasVnrArea))' = ifelse(
      MasVnrArea == 0,
      0,
      Winsorize(
        x = `cbrt(MasVnrArea)`,
        # probs = c(0.005, 0.995),
        minval = min_val,
        maxval = max_val
      )
    )
  )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(cbrt(MasVnrArea))'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('MasVnrArea', 'cbrt(MasVnrArea)', x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
 plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst) 
}
```

This variable has a lot of zeros that indicate a missing feature that make it a candidate for binary encoding to aid linear regression. But, MasVnrType already encodes that in 'None'. Because there's a significant difference in price between MasVnrType levels, that should suffice. Also, leaving the 0s in improves the correlation to the target variable, so it may be moot.

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
# already hardcoded this one.

print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

ggplot(val_train_Xy, aes(x = .data[[x]])) +
  geom_histogram(binwidth = .2)
```

### By Factors

```{r echo=TRUE, message=FALSE, warning=FALSE}
y_lst = c('MasVnrType')
z = 'SalePrice.fact'
for (y in y_lst) {
  plt = fenced_jbv(
    data = val_train_Xy,
    x = y,
    y = 'cbrt(MasVnrArea)',
    jit_col = z,
    jit_alpha = 0.75,
    leg_lb = z,
    box_color = 'red'
  ) +
    theme(axis.text.x = element_text(angle = 45, hjust=1))
  print(plt)
}

ggplot(val_train_Xy, aes(x = `cbrt(MasVnrArea)`, y = `log(SalePrice)`)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm') +
  facet_wrap(vars(MasVnrType))
```
There are some houses with masonry footage but no type ascribed. I noticed this during the wrangling audit and decided to leave it for the modeling phase, maybe to impute the masonry type with caret using a select few variables as indicators or simply imputing the most common type (BrkFace), but maybe create a new level for those handful of undescribed masonry types.

## ExterQual

[Back to top.](#top)

Mostly average (440), many good (241).

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'ExterQual'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## ExterCond

[Back to top.](#top)

Greater majority average (621), still some good (74). May be worth keeping.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'ExterCond'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## Foundation

[Back to top.](#top)

Evenly split between poured concrete and cinder block (317 and 302), but still 72 brick and tile.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Foundation'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Could drop Stone and Wood and make it an ordered factor to represent as ints in regression or to one-hot.

## BsmtQual

[Back to top.](#top)

Evenly split between average and good (313 and 303), but 63 excellent, and only 26 without basements. I'm having trouble imagining houses with basements and cinder block foundations.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtQual'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## BsmtCond

[Back to top.](#top)

Vast majority average (635).

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtCond'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## BsmtExposure

[Back to top.](#top)

Great majority (about 464) not exposed, but still some average (100), good (71), and minimal exposure (54).

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtExposure'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## BsmtFinType1

[Back to top.](#top)

211 are unfinished, 205 are top quality, and descending counts to low quality (34).

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtFinType1'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Probably just one-hot GLQ, LwQ, and None for regression.

## BsmtFinSF1 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtFinSF1'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$BsmtFinSF1 != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 50,
  t_binw = 0.25
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(BsmtFinSF1)'
val_train_Xy = val_train_Xy %>%
  mutate('cbrt(BsmtFinSF1)' = BsmtFinSF1^(1/3))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$BsmtFinSF1 != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.25,
  t_binw = 0.25
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(val_train_Xy, aes(x = .data[['cbrt(BsmtFinSF1)']])) +
  geom_histogram()
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('BsmtFinSF1', 'cbrt(BsmtFinSF1)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
plot_scat_pairs(df = val_train_Xy, x = 'BsmtFinSF1', y_lst = y_lst)
```

This will be dropped in favor of a derivative variable, total basement sf, so no need to go any further.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(val_train_Xy, -c('cbrt(BsmtFinSF1)'))
```

## BsmtFinType2

[Back to top.](#top)

Mostly unfinished (609) but 27 no basements. So, only one house in Ames with a basement doesn't have a second basement?? This doesn't seem right. It might be worth dropping this feature.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtFinType2'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## BsmtFinSF2 

[Back to top.](#top)

No need to look further as this will be dropped in favor of total bsmt sf.

## BsmtUnfSF 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BsmtUnfSF'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 50,
  t_binw = 0.25
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(BsmtUnfSF)'
val_train_Xy = val_train_Xy %>%
  mutate('cbrt(BsmtUnfSF)' = BsmtUnfSF^(1/3))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = filter(val_train_Xy, BsmtUnfSF != 0),
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.25,
  t_binw = 0.25
)
```

### Winsorize

Because this variable's 0s indicate a missing basement, just Winsorizing non-zero values.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = val_train_Xy[val_train_Xy$BsmtUnfSF != 0, ]

qqnorm(y = df$BsmtUnfSF, ylab = 'BsmtUnfSF')
qqline(y = df$BsmtUnfSF, ylab = 'BsmtUnfSF')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

Win_cbrt_x = Winsorize(
  x = df[[x]],
  probs = c(0.05, 0.95),
  na.rm = T
)

qqnorm(y = Win_cbrt_x, ylab = 'Win(cbrt(BsmtUnfSF)')
qqline(y = Win_cbrt_x, ylab = 'Win(cbrt(BsmtUnfSF)')

Win_raw_x = Winsorize(
  x = df$BsmtUnfSF,
  probs = c(0, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'BsmtUnfSF')
qqline(y = Win_raw_x, ylab = 'BsmtUnfSF')

print(shapiro.test(x = df$BsmtUnfSF))
print(shapiro.test(x = df$`cbrt(BsmtUnfSF)`))
print(shapiro.test(x = Win_cbrt_x))
print(shapiro.test(x = Win_raw_x))
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(BsmtUnfSF)'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('BsmtUnfSF', 'cbrt(BsmtUnfSF)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

## TotalBsmtSF 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'TotalBsmtSF'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 50,
  t_binw = 1/20
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'log(TotalBsmtSF)'
val_train_Xy = val_train_Xy %>%
  mutate(
    'log(TotalBsmtSF)' = ifelse(
    TotalBsmtSF <= 0,
    0,
    log(TotalBsmtSF)
    )
  )

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1/20,
  t_binw = 1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'square(log(TotalBsmtSF))'
val_train_Xy = val_train_Xy %>%
  mutate(
    'square(log(TotalBsmtSF))' = ifelse(
    TotalBsmtSF <= 0,
    0,
    log(TotalBsmtSF)^2
    )
  )

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

### Winsorize

Just checking non-zero set.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = val_train_Xy[val_train_Xy$TotalBsmtSF != 0, ]

qqnorm(y = df$TotalBsmtSF, ylab = 'TotalBsmtSF')
qqline(y = df$TotalBsmtSF, ylab = 'TotalBsmtSF')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

Win_log_x_squared = Winsorize(
  x = df[[x]],
  probs = c(0.005, 0.995),
  na.rm = T
)

qqnorm(y = Win_log_x_squared, ylab = 'Win_log_x_squared')
qqline(y = Win_log_x_squared, ylab = 'Win_log_x_squared')

Win_raw_x = Winsorize(
  x = df$TotalBsmtSF,
  probs = c(0, 0.99),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = df$TotalBsmtSF))
print(shapiro.test(x= df$`square(log(TotalBsmtSF))`))
print(shapiro.test(x = Win_log_x_squared))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(square(log(TotalBsmtSF)))'

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(square(log(TotalBsmtSF)))' = ifelse(
      TotalBsmtSF <= 0,
      0,
      Winsorize(
        x = log(TotalBsmtSF)^2,
        probs = c(0.005, 0.995),
        na.rm = T
      )
    )
  ) %>%
  mutate(
    'Win(log(TotalBsmtSF))' = ifelse(
      TotalBsmtSF <= 0,
      0,
      Winsorize(
        x = log(TotalBsmtSF),
        probs = c(0.005, 0.995),
        na.rm = T
      )
    )
  )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('TotalBsmtSF', 'log(TotalBsmtSF)', 'square(log(TotalBsmtSF))',
          'Win(square(log(TotalBsmtSF)))')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features') +
  xlab(label = 'Subset with no 0s')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

Looks to be some clustering here, two or three groups. Maybe those with and without a second basement, maybe basement types. I think regression will suss out what information is needed, but it's worth noting.

The raw feature actually correlates much better with the target variable when 0s are included. It might be worth keeping the raw features for the Lasso regression to weed through. Normalizing apart from 0s may or may not help other regressions; it would be ideal for some kind of combo regression that splits/clusters then finds a linear regression, which is sort of what I'm attempting by feeding the binary version of these features to Lasso (where not already encoded by a factor). Normalizing apart for 0s is not likely to help decision trees much, but may help KNN a little by pulling 0s farther away and outliers closer.

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(square(log(TotalBsmtSF)))'

min_val = min(Win_log_x_squared)
max_val = max(Win_log_x_squared)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(square(log(TotalBsmtSF)))' = ifelse(
      TotalBsmtSF == 0,
      0,
      Winsorize(
        log(TotalBsmtSF)^2,
        # probs = c(0.005, 0.995),
        minval = min_val,
        maxval = max_val
      )
    )
  ) %>%
  select(-c('Win(log(TotalBsmtSF))', 'log(TotalBsmtSF)'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

### Binarize

I can create a binary for basement when I one-hot encode during modeling, but I want to be able to use it for exploration here.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('Bsmt.bin' = factor(ifelse(TotalBsmtSF == 0, 0, 1), ordered = T))

x = 'Bsmt.bin'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 20)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(val_train_Xy, -c('Bsmt.bin'))
```

## TotalBsmtFinSF 

[Back to top.](#top)

TotalBsmtSF - BsmtUnfSF 

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'TotalBsmtFinSF'
val_train_Xy = val_train_Xy %>%
  mutate('TotalBsmtFinSF' = TotalBsmtSF - BsmtUnfSF)

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])

sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy[[x]] != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 50,
  t_binw = 1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(TotalBsmtFinSF)'
val_train_Xy = val_train_Xy %>%
  mutate('sqrt(TotalBsmtFinSF)' = sqrt(TotalBsmtFinSF))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])

sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy[[x]] != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

### Winsorize

Just the non-zero set.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(TotalBsmtFinSF)'
df = val_train_Xy[val_train_Xy$TotalBsmtFinSF != 0, ]

qqnorm(y = df$TotalBsmtFinSF, ylab = 'TotalBsmtFinSF')
qqline(y = df$TotalBsmtFinSF, ylab = 'TotalBsmtFinSF')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)


Win_sqrt_x = Winsorize(
  x = df[[x]],
  probs = c(0.03, 0.995),
  na.rm = T
)

qqnorm(y = Win_sqrt_x, ylab = 'Win_sqrt_x')
qqline(y = Win_sqrt_x, ylab = 'Win_sqrt_x')


Win_raw_x = Winsorize(
  x = df$TotalBsmtFinSF,
  probs = c(0.001, 0.99),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win(TotalBsmtFinSF)')
qqline(y = Win_raw_x, ylab = 'Win(TotalBsmtFinSF)')


print(shapiro.test(x = df$TotalBsmtFinSF))
print(shapiro.test(x = df$`sqrt(TotalBsmtFinSF)`))
print(shapiro.test(x = Win_sqrt_x))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(sqrt(TotalBsmtFinSF))' = ifelse(
      TotalBsmtFinSF == 0,
      0,
      Winsorize(
        x = sqrt(TotalBsmtFinSF),
        # probs = c(0.01, 0.995),
        minval = min(Win_sqrt_x),
        maxval = max(Win_sqrt_x)
      )
    )
  ) %>%
  mutate(
    'Win(TotalBsmtFinSF)' = ifelse(
      TotalBsmtFinSF == 0,
      0,
      Winsorize(
        x = TotalBsmtFinSF,
        # probs = c(0.001, 0.99)
        minval = min(Win_raw_x),
        maxval = max(Win_raw_x)
      )
    )
  )

x = 'Win(sqrt(TotalBsmtFinSF))'
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('TotalBsmtFinSF', 'sqrt(TotalBsmtFinSF)', x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(sqrt(TotalBsmtFinSF))'

min_val = min(Win_sqrt_x)
max_val = max(Win_sqrt_x)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

# Already hard coded above.
val_train_Xy = select(val_train_Xy, -c('Win(TotalBsmtFinSF)'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

## Heating

[Back to top.](#top)

Mostly gas forced air (GasA, 697). Can probably drop this.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$Heating)
val_train_Xy = select(val_train_Xy, -c('Heating'))
```

## HeatingQC

[Back to top.](#top)

Mostly excellent (360), then interestingly more average (211) than good (117). Enough variance to keep, but not normal.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'HeatingQC'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## CentralAir

[Back to top.](#top)

662 yes, 53 no. Probably drop this one.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$CentralAir)
val_train_Xy = select(val_train_Xy, -c('CentralAir'))
```

## Electrical

[Back to top.](#top)

655 standard circuit breaker and Romex. Probably drop, but there are none mixed, so it might be worth making it an ordered factor if keeping.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$Electrical)
val_train_Xy = select(val_train_Xy, -c('Electrical'))
```

## X1stFlrSF

[Back to top.](#top)

Dropping as component of GrLivArea.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$X1stFlrSF)
val_train_Xy = select(val_train_Xy, -c('X1stFlrSF'))
```

## X2ndFlrSF

[Back to top.](#top)

Dropping as component of GrLivArea. The presence of a second floor is encoded in HouseStyle, but it's also convoluted between a few levels. I'll make a binary out of it and keep that.

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('X2ndFlr.bin' = ifelse(X2ndFlrSF <= 0, 0, 1)) %>%
  mutate('X2ndFlr.bin.fact' = factor(X2ndFlr.bin, ordered = T))

x = 'X2ndFlr.bin'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
x = 'X2ndFlr.bin.fact'
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

val_train_Xy = select(val_train_Xy, -c('X2ndFlr.bin.fact'))
```

## LowQualFinSF

[Back to top.](#top)

702 0s. Drop this feature, though it would be a useful modifier of GrLivArea as a detracting component of it.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$LowQualFinSF)
val_train_Xy = select(val_train_Xy, -c('LowQualFinSF'))
```

## GrLivArea 

[Back to top.](#top)

### Normalize

As this is polymodal to the number of floors, I'll start by faceting before transforming.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(val_train_Xy, aes(x = GrLivArea)) +
  geom_histogram(binwidth = 50) +
  facet_wrap(facets = vars(X2ndFlr.bin), ncol = 1)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GrLivArea'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 50,
  t_binw = .1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'log2(GrLivArea)'

val_train_Xy = val_train_Xy %>%
  mutate('log2(GrLivArea)' = log2(GrLivArea))

ggplot(val_train_Xy, aes(x = .data[[x]])) +
  geom_histogram(binwidth = .1) +
  facet_wrap(facets = vars(X2ndFlr.bin), ncol = 1)

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = .1,
  t_binw = 1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'square(log2(GrLivArea))'

val_train_Xy = val_train_Xy %>%
  mutate('square(log2(GrLivArea))' = log2(GrLivArea)^2)

ggplot(val_train_Xy, aes(x = .data[[x]])) +
  geom_histogram(binwidth = 1) +
  facet_wrap(facets = vars(X2ndFlr.bin), ncol = 1)

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(val_train_Xy, aes(x = `square(log2(GrLivArea))`)) +
  geom_histogram(binwidth = 1) +
  facet_wrap(facets = vars(.data$X2ndFlr.bin), ncol = 1)

ggplot(val_train_Xy, aes(x = `square(log2(GrLivArea))`)) +
  geom_histogram(binwidth = 1) +
  facet_grid(
    cols = vars(.data$X2ndFlr.bin),
    rows = vars(.data$YearBuilt.fact)
  )
```

It looks like there might be more polymodality, particularly among mid-century builds, but I'll leave it here. The transformations seem to have better normalized both the full variable and its subsets by storey and age. Winsorization should help both too.

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'square(log2(GrLivArea))'

qqnorm(y = val_train_Xy$GrLivArea, ylab = 'GrLivArea')
qqline(y = val_train_Xy$GrLivArea, ylab = 'GrLivArea')

qqnorm(y = val_train_Xy$`log2(GrLivArea)`, ylab = 'log2(GrLivArea)')
qqline(y = val_train_Xy$`log2(GrLivArea)`, ylab = 'log2(GrLivArea)')

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

Win_log2_x_squared = Winsorize(
  x = val_train_Xy[[x]],
  probs = c(0.002, 0.998),
  na.rm = T
)

qqnorm(y = Win_log2_x_squared, ylab = 'Win_log2_x_squared')
qqline(y = Win_log2_x_squared, ylab = 'Win_log2_x_squared')

Win_log2_x = Winsorize(
  x = val_train_Xy$`log2(GrLivArea)`,
  probs = c(0.002, 0.998),
  na.rm = T
)

qqnorm(y = Win_log2_x, ylab = 'Win_log2_x')
qqline(y = Win_log2_x, ylab = 'Win_log2_x')

Win_raw_x = Winsorize(
  x = val_train_Xy$GrLivArea,
  probs = c(0, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy$GrLivArea))
print(shapiro.test(x = val_train_Xy$`log2(GrLivArea)`))
print(shapiro.test(x = val_train_Xy[['square(log2(GrLivArea))']]))
print(shapiro.test(x = Win_log2_x_squared))
print(shapiro.test(x = Win_log2_x))
print(shapiro.test(x = Win_raw_x))
```

Winsorizing the log2 better normalized than squaring it, but Winsorizing the squared log2 is close, too. May be overfit to add the square.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(log2(GrLivArea))' = Winsorize(
      ifelse(GrLivArea <= 0, 0, log2(GrLivArea)),
      probs = c(0.002, 0.998),
      na.rm = T
    )
  ) %>%
  mutate(
    'Win(square(log2(GrLivArea)))' = Winsorize(
      ifelse(GrLivArea <= 0, 0, log2(GrLivArea)^2),
      probs = c(0.002, 0.998),
      na.rm = T
    )
  )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('GrLivArea', 'log2(GrLivArea)', 'square(log2(GrLivArea))', 'Win(log2(GrLivArea))', 'Win(square(log2(GrLivArea)))')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

The Winsorized double transformation may be slightly overfitting and overcomputing, but it's slightly (likely insignificantly) more normal and correlated to SalePrice than the Winsorized single transformation. I'll go with it despite common sense, just for fun.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(square(log2(GrLivArea)))'

min_val = min(Win_log2_x_squared)
max_val = max(Win_log2_x_squared)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(square(log2(GrLivArea)))' = Winsorize(
      ifelse(GrLivArea <= 0, 0, log2(GrLivArea)^2),
      # probs = c(0.002, 0.998),
      # na.rm = T
      minval = min_val,
      maxval = max_val
    )
  ) %>%
  select(-c('log2(GrLivArea)', 'Win(log2(GrLivArea))'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

## BsmtFullBath

[Back to top.](#top)

To contribute to full count.

## BsmtHalfBath

[Back to top.](#top)

To contribute to full count.

## FullBath

[Back to top.](#top)

To contribute to full count.

## HalfBath

[Back to top.](#top)

To contribute to full count.

## TotBaths

[Back to top.](#top)

I could make two features, total baths above grade and total basement baths, but I'll keep it simple.

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(TotBaths = FullBath + BsmtFullBath + 0.5*HalfBath + 0.5*BsmtHalfBath)

x = 'TotBaths'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)
```

<a id="totbathBsmtbath"></a>

```{r echo=TRUE, warning=FALSE, message=FALSE}

ggplot(val_train_Xy, aes(x = factor(TotBaths), y = log(SalePrice))
) +
  geom_jitter(alpha = 0.5, aes(color = factor(BsmtFullBath + BsmtHalfBath))) +
  geom_boxplot(
    notch = T,
    notchwidth = .1,
    varwidth = T,
    alpha = 0,
    color = 'blue'
  ) +
  geom_violin(alpha = 0) +
  geom_line(
    stat = 'summary',
    fun = quantile,
    fun.args = list(probs = .9),
    linetype = 2, aes(group = 1)
  ) +
  geom_line(stat = 'summary', fun = mean, mapping = aes(group = 1)) +
  geom_line(
    stat = 'summary',
    fun = quantile,
    fun.args = list(probs = .1),
    linetype = 2, aes(group = 1)
  ) +
  xlab(label = 'TotBaths') + ylab(label = 'log(SalePrice)')
```

Basement bathrooms don't seem to add much value when there aren't many bathrooms altogether.

```{r echo=TRUE, warning=FALSE, message=FALSE}
p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.5,
  t_binw = 0.1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(TotBaths)'
val_train_Xy = val_train_Xy %>%
  mutate('sqrt(TotBaths)' = sqrt(TotBaths))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = .1,
  t_binw = .1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(TotBaths)'

qqnorm(y = val_train_Xy$TotBaths, ylab = 'TotBaths')
qqline(y = val_train_Xy$TotBaths, ylab = 'TotBaths')

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

Win_sqrt_x = Winsorize(
  x = val_train_Xy[[x]],
  probs = c(0.0, 0.999),
  na.rm = T
)

qqnorm(y = Win_sqrt_x, ylab = 'Win_sqrt_x')
qqline(y = Win_sqrt_x, ylab = 'Win_sqrt_x')

Win_raw_x = Winsorize(
  x = val_train_Xy$TotBaths,
  probs = c(0, 0.998),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy$TotBaths))
print(shapiro.test(x = val_train_Xy$`sqrt(TotBaths)`))
print(shapiro.test(x = Win_sqrt_x))
print(shapiro.test(x = Win_raw_x))
```

Maybe just lightly top-coding the raw variable is best.


```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(sqrt(TotBaths))' = Winsorize(
      sqrt(TotBaths),
      probs = c(0, 0.999),
      na.rm = T
    )
  ) %>%
  mutate(
    'Win(TotBaths)' = Winsorize(
      TotBaths,
      probs = c(0, 0.998),
      na.rm = T
    )
  )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(TotBaths)'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('TotBaths', 'sqrt(TotBaths)', 'Win(sqrt(TotBaths))', 'Win(TotBaths)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(TotBaths)'

min_val = min(Win_raw_x)
max_val = max(Win_raw_x)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(TotBaths)' = Winsorize(
      FullBath + BsmtFullBath + 0.5*HalfBath + 0.5*BsmtHalfBath,
      # probs = c(0.002, 0.998),
      # na.rm = T
      minval = min_val,
      maxval = max_val
    )
  ) %>%
  select(-c('sqrt(TotBaths)', 'Win(sqrt(TotBaths))'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 0.5)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

## BedroomAbvGr 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'BedroomAbvGr'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'

df = select(val_train_Xy, c('log(SalePrice)', 'BedroomAbvGr'))
df$BedroomAbvGr.fact = factor(df$BedroomAbvGr)
sum_and_trans_fact(data = df, x = 'BedroomAbvGr.fact', y = y)

p_vals = get_signif_levels(
  data = df,
  x = 'BedroomAbvGr.fact',
  z = y,
  min_n = 30
)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)


num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('BedroomAbvGr')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.5,
  t_binw = 0.1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(BedroomAbvGr)'
val_train_Xy = val_train_Xy %>%
  mutate('sqrt(BedroomAbvGr)' = sqrt(BedroomAbvGr))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = .1,
  t_binw = .1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(BedroomAbvGr)'

qqnorm(y = val_train_Xy$BedroomAbvGr, ylab = 'BedroomAbvGr')
qqline(y = val_train_Xy$BedroomAbvGr, ylab = 'BedroomAbvGr')

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

Win_sqrt_x = Winsorize(
  x = val_train_Xy[[x]],
  probs = c(0, 0.999),
  na.rm = T
)

qqnorm(y = Win_sqrt_x, ylab = 'Win_sqrt_x')
qqline(y = Win_sqrt_x, ylab = 'Win_sqrt_x')

Win_raw_x = Winsorize(
  x = val_train_Xy$BedroomAbvGr,
  probs = c(0, 0.995),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy$BedroomAbvGr))
print(shapiro.test(x = val_train_Xy$`sqrt(BedroomAbvGr)`))
print(shapiro.test(x = Win_sqrt_x))
print(shapiro.test(x = Win_raw_x))
```

Maybe just lightly Winsorizing the raw variable is best.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(sqrt(BedroomAbvGr))' = Winsorize(
      sqrt(BedroomAbvGr),
      probs = c(0, 0.999),
      na.rm = T
    )
  ) %>%
  mutate(
    'Win(BedroomAbvGr)' = Winsorize(
      BedroomAbvGr,
      probs = c(0, 0.995),
      na.rm = T
    )
  )
```


### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(BedroomAbvGr)'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('BedroomAbvGr', 'sqrt(BedroomAbvGr)', 'Win(sqrt(BedroomAbvGr))', 'Win(BedroomAbvGr)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(BedroomAbvGr)'

min_val = min(Win_raw_x)
max_val = max(Win_raw_x)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(BedroomAbvGr)' = Winsorize(
      BedroomAbvGr,
      # probs = c(0.002, 0.998),
      # na.rm = T
      minval = min_val,
      maxval = max_val
    )
  ) %>%
  select(-c('sqrt(BedroomAbvGr)', 'Win(sqrt(BedroomAbvGr))'))

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

## KitchenAbvGr

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$KitchenAbvGr)
val_train_Xy = select(val_train_Xy, -c('KitchenAbvGr'))
```

## KitchenQual

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'KitchenQual'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## TotRmsAbvGrd

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'TotRmsAbvGrd'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'

df = select(val_train_Xy, c('log(SalePrice)', 'TotRmsAbvGrd'))
df$TotRmsAbvGrd.fact = factor(df$TotRmsAbvGrd)
sum_and_trans_fact(data = df, x = 'TotRmsAbvGrd.fact', y = y)

p_vals = get_signif_levels(
  data = df,
  x = 'TotRmsAbvGrd.fact',
  z = y,
  min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)

num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('TotRmsAbvGrd')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'TotRmsAbvGrd'

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

Win_raw_x = Winsorize(
  x = val_train_Xy$TotRmsAbvGrd,
  probs = c(0, 0.975),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win(TotRmsAbvGrd)')
qqline(y = Win_raw_x, ylab = 'Win(TotRmsAbvGrd)')

print(shapiro.test(x = val_train_Xy$TotRmsAbvGrd))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(TotRmsAbvGrd)' = Winsorize(
      TotRmsAbvGrd,
      probs = c(0, 0.975),
      na.rm = T
    )
  )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(TotRmsAbvGrd)'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('TotRmsAbvGrd', 'Win(TotRmsAbvGrd)')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(TotRmsAbvGrd)'

min_val = min(Win_raw_x)
max_val = max(Win_raw_x)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(TotRmsAbvGrd)' = Winsorize(
      TotRmsAbvGrd,
      # probs = c(0, 0.975),
      # na.rm = T
      minval = min_val,
      maxval = max_val
    )
  )

gg = ggplot(val_train_Xy, aes(x = .data[[x]]))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

## Functional

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}

x = 'Functional'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## Fireplaces 

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Fireplaces'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)

val_train_Xy$Fireplaces.fact = factor(val_train_Xy$Fireplaces, ordered = T)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = 'Fireplaces.fact', y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = 'Fireplaces.fact', z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('Fireplaces')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

plot_scat_pairs(df = val_train_Xy, x = x, y_lst = c(y))
```

The variable needs more than three unique values in order to use my function to check for transformations, and 0 can't be one of them in order to include log transformations. So, I'll use Fireplaces + 1 to search for transformations.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy$Fireplaces_plus1 = val_train_Xy$Fireplaces + 1
x = 'Fireplaces_plus1'

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Fireplaces'

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

Win_raw_x = Winsorize(
  x = val_train_Xy$Fireplaces,
  probs = c(0, 0.999),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy$Fireplaces))
print(shapiro.test(x = Win_raw_x))
```

Winsorization doesn't help. 

### Binarize

There's no apparent meaningful difference in price between houses with 1 fireplace and those with 2 or 3. It's significant (p < 0.1), but not apparently meaningful given the overlapping notches in the boxplot above, though I did not take Cohen's d for effect size.

Binarization may condense the feature space without a lot of information loss. In fact, it improves the correlation to the target variable.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(Fireplaces.bin = ifelse(Fireplaces == 0, 0, 1))

x = 'Fireplaces.bin'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)

val_train_Xy$Fireplaces.bin.fact = factor(val_train_Xy$Fireplaces.bin, ordered = T)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = 'Fireplaces.bin.fact', y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = 'Fireplaces.bin.fact', z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('Fireplaces', 'Fireplaces.bin')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy,
  -c('Fireplaces.fact', 'Fireplaces_plus1', 'Fireplaces.bin.fact',
     'Fireplaces')
)
```

## FireplaceQu

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'FireplaceQu'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 29)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

One-hot encoding this FireplaceQu will include a binarized version ("None":int[0,1]), so we can drop Fireplace.bin altogether, and Fireplace because it just adds noise. I'll take care of that during modeling with caret.

## GarageType

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageType'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## GarageYrBlt 

[Back to top.](#top)

Similar distribution as year built for house. May not add additional information, but may be useful in interaction with type and finish. Could drop and use YearBuilt as proxy but would lose some info.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageYrBlt'
summary(val_train_Xy[x])
# sum_and_trans_cont(
#   data = val_train_Xy,
#   x = x,
#   func = best_normalizers[[x]]$best_func$func,
#   func_name = best_normalizers[[x]]$best_func$name,
#   x_binw = 1,
#   t_binw = 1
# )

gg = ggplot(val_train_Xy, aes(x = GarageYrBlt))
p1 = gg + geom_histogram(binwidth = 1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageYrBlt'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c(x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

<a id="garbltControl"></a>

### "Controlling"

Even "controlling" for the year the house was built, GarageYrBlt doesn't predict sale prices well, as seen in the next few plots. I'll just drop it.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'GarBltLater' = factor(ifelse((GarageYrBlt - YearBuilt) <= 0, 0, 1))
  )

ggplot(val_train_Xy, aes(x = GarageYrBlt, y = YearBuilt)) +
  geom_jitter(
    alpha = 0.5,
    aes(
      color = SalePrice.fact
    ),
    position = position_jitter(w = 1, h = 1)
  )

fbv = fenced_jbv(
  data = val_train_Xy,
  x = 'GarBltLater',
  y = 'log(SalePrice)'
)
fbv

fbv +
  facet_wrap(facets = vars(YearBuilt.fact))

val_train_Xy$resids = lm(
  `log(SalePrice)` ~ `sqrt(Age)`,
  val_train_Xy
)$residuals

df = filter(
  val_train_Xy,
  GarageYrBlt - YearBuilt != 0
)

ggplot(df, aes(x = GarageYrBlt, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  ylab(label = 'log(SalePrice) ~ YearBuilt Residuals')

print(
  paste(
    "Correlation of GarageYRBlt to Age residual:",
    cor(df$GarageYrBlt, df$resids)
  )
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(val_train_Xy, -c('GarageYrBlt', 'GarBltLater'))
```

## GarageFinish

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageFinish'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## GarageCars 

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageCars'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)

y = 'log(SalePrice)'
val_train_Xy$GarageCars.fact = factor(
  val_train_Xy$GarageCars,
  ordered = T
)
sum_and_trans_fact(data = val_train_Xy, x = 'GarageCars.fact', y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = 'GarageCars.fact', z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c(x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

plot_scat_pairs(df = val_train_Xy, x = x, y_lst = c(y))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy$GarageCars.plus1 = val_train_Xy$GarageCars + 1
x = 'GarageCars.plus1'

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageCars'

qqnorm(y = val_train_Xy[[x]], ylab = x)
qqline(y = val_train_Xy[[x]], ylab = x)

val_train_Xy$`Win(GarageCars)` = Winsorize(
  x = val_train_Xy[[x]],
  probs = c(0, 0.999),
  na.rm = T
)

qqnorm(y = val_train_Xy$`Win(GarageCars)`, ylab = 'Win_raw_x')
qqline(y = val_train_Xy$`Win(GarageCars)`, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy[[x]]))
print(shapiro.test(x = val_train_Xy$`Win(GarageCars)`))
```

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(GarageCars.bin = ifelse(GarageCars == 0, 0, 1))

x = 'GarageCars.bin'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)

val_train_Xy$GarageCars.bin.fact =
  factor(val_train_Xy$GarageCars.bin, ordered = T)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = 'GarageCars.bin.fact', y = y)

p_vals = get_signif_levels(
  data = val_train_Xy, x = 'GarageCars.bin.fact', z = y, min_n = 30
)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)

num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('GarageCars', 'Win(GarageCars)', 'GarageCars.bin')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')
```

That didn't seem to help.

## GarageArea 

[Back to top.](#top)

### Normalize

Polymodal in interaction with GarageCars.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'GarageArea'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 50,
  t_binw = 1
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(GarageArea)'
val_train_Xy = val_train_Xy %>%
  mutate('sqrt(GarageArea)' = sqrt(GarageArea))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy,
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 1,
  t_binw = 1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'sqrt(GarageArea)'
df = filter(val_train_Xy, GarageArea != 0)

qqnorm(y = df$GarageArea, ylab = 'GarageArea')
qqline(y = df$GarageArea, ylab = 'GarageArea')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

Win_sqrt_x = Winsorize(
  x = df[[x]],
  probs = c(0, 0.998),
  na.rm = T
)

qqnorm(y = Win_sqrt_x, ylab = 'Win_sqrt_x')
qqline(y = Win_sqrt_x, ylab = 'Win_sqrt_x')

Win_raw_x = Winsorize(
  x = df$GarageArea,
  probs = c(0, 0.987),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = val_train_Xy$GarageArea))
print(shapiro.test(x = val_train_Xy$`sqrt(GarageArea)`))
print(shapiro.test(x = Win_sqrt_x))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
min_val = min(Win_sqrt_x)
max_val = max(Win_sqrt_x)
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(sqrt(GarageArea))' = ifelse(
      GarageArea == 0,
      0,
      Winsorize(
        sqrt(GarageArea),
        minval = min_val,
        maxval = max_val
      )
    )
  )
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(sqrt(GarageArea))'
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('GarageArea', 'sqrt(GarageArea)', x)

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
}
```

Transforming and Winsorizing doesn't help the overall correlation to SalePrice. Let's facet it out and see.

### Garage Area and Cars by Type

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = filter(
  val_train_Xy,
  (GarageType %in% c('Attchd', 'Detchd', 'BuiltIn')) &
    (GarageCars != 4) # Ony one point in this subset with 4 and it doesn't really change the correlations, but it saves viz space.
)

ggplot(df, aes(x = `Win(sqrt(GarageArea))`, y = `log(SalePrice)`)) +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = 'lm') +
  facet_grid(rows = vars(GarageType), cols = vars(GarageCars.fact))

sum_df0 = df %>%
  summarize(
    n = n(),
    GarageArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`GarageArea`
    ),
    sqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`sqrt(GarageArea)`
    ),
    WinsqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(sqrt(GarageArea))`
    )
  )

min_n = 30

# sum_df1 = df %>%
#   group_by(GarageCars.fact, GarageType) %>%
#   summarize(
#     n = n(),
#     GarageArea_cor = cor(
#       x = .data$`Win(log(SalePrice))`,
#       y = .data$`GarageArea`
#     ),
#     sqrtArea_cor = cor(
#       x = .data$`Win(log(SalePrice))`,
#       y = .data$`sqrt(GarageArea)`
#     ),
#     WinsqrtArea_cor = cor(
#       x = .data$`Win(log(SalePrice))`,
#       y = .data$`Win(sqrt(GarageArea))`
#     )
#   ) %>%
#   filter(n >= min_n)

sum_df2 = df %>% group_by(GarageType) %>%
  summarize(
    n = n(),
    GarageArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`GarageArea`
    ),
    sqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`sqrt(GarageArea)`
    ),
    WinsqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(sqrt(GarageArea))`
    )
  ) %>%
  filter(n >= min_n)

sum_df3 = df %>% group_by(GarageCars.fact) %>%
  summarize(
    n = n(),
    GarageArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`GarageArea`
    ),
    sqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`sqrt(GarageArea)`
    ),
    WinsqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(sqrt(GarageArea))`
    )
  ) %>%
  filter(n >= min_n)

sum_df0 %>%
  merge(y = sum_df2, all = T) %>%
  merge(y = sum_df3, all = T) %>%
  # merge(y = sum_df1, all = T) %>%
  select(
    c('GarageType', 'GarageCars.fact', 'n', 'GarageArea_cor',
      'sqrtArea_cor', 'WinsqrtArea_cor')
  ) %>%
  arrange(GarageType, GarageCars.fact)
```

Transforming and Winsorizing GarageArea improves correlation to the target variable ("Win(log(SalePrice))") somewhat when excluding houses without garages. Grouping by garage type helps in some cases.

Whether to transform the variable or not may depend on which ML algorithm we're using and how. A decision tree will likely be indifferent to tranformations, though it may benefit from noise reduction with Winsorization. A linear regression will only benefit if type and/or missingness is factored in, as would KNN.

Grouping by number of cars lowers the area:price correlation to no correlation. And, in fact, number of cars has a stronger correlation to the target variable than area does. Let's see how grouping by type further improves that correlation.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = filter(
  val_train_Xy,
  (GarageType %in% c('Attchd', 'Detchd', 'BuiltIn'))
)

ggplot(df, aes(x = GarageCars, y = `log(SalePrice)`)) +
  geom_jitter(alpha = 0.5) +
  geom_smooth(method = 'lm') +
  facet_wrap(facets = vars(GarageType))

sum_df0 = df %>%
  summarize(
    n = n(),
    Cars_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$GarageCars
    ),
    WinCars_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(GarageCars)`
    ),
    WinsqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(sqrt(GarageArea))`
    )
  )

sum_df2 = df %>% group_by(GarageType) %>%
  summarize(
    n = n(),
    Cars_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$GarageCars
    ),
    WinCars_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(GarageCars)`
    ),
    WinsqrtArea_cor = cor(
      x = .data$`Win(log(SalePrice))`,
      y = .data$`Win(sqrt(GarageArea))`
    )
  )

sum_df0 %>%
  merge(y = sum_df2, all = T) %>%
  select(
    c('GarageType', 'n', 'Cars_cor', 'WinCars_cor', 'WinsqrtArea_cor')
  ) %>%
  arrange(GarageType)
```

It looks worth dropping GarageArea in favor of GarageCars. There is an argument against linear regression using discrete variables, but it seems to work nonetheless.

Winsorizing GarageCars only produces a marginal benefit which may prove spurious as it only adjusts one point. Plus, it reduces normality. So, I'll just use the raw feature.

There are likely other features at play, like year built, that also affect things like the difference in the prices of two-car attached garages and two-car detached garages.

```{r echo=TRUE, message=FALSE, warning=FALSE}
fenced_jbv(
  data = df[df$GarageCars != 4, ],
  x = 'GarageType',
  y = 'log(SalePrice)',
  jit_col = 'YearBuilt.fact',
  leg_lbl = 'Year Built'
) +
  facet_wrap(facets = vars(GarageCars.fact))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy,
  -c('GarageCars.fact', 'GarageCars.plus1', 'GarageCars.bin',
     'GarageCars.bin.fact', 'GarageArea', 'sqrt(GarageArea)',
     'Win(sqrt(GarageArea))')
)
```

## GarageQual, Cond

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy[ , c('GarageQual', 'GarageCond')])
val_train_Xy = select(val_train_Xy, -c('GarageQual', 'GarageCond'))
```

## PavedDrive

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$PavedDrive)
val_train_Xy = select(val_train_Xy, -c('PavedDrive'))
```

## WoodDeckSF

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'WoodDeckSF'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$WoodDeckSF != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 10,
  t_binw = .1
)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(WoodDeckSF)'
val_train_Xy = val_train_Xy %>%
  mutate('cbrt(WoodDeckSF)' = (WoodDeckSF)^(1/3))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy[[x]] != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = .1,
  t_binw = .1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = filter(val_train_Xy, WoodDeckSF != 0)

qqnorm(y = df$WoodDeckSF, ylab = 'WoodDeckSF')
qqline(y = df$WoodDeckSF, ylab = 'WoodDeckSF')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

Win_cbrt_x = Winsorize(
  x = df[[x]],
  probs = c(0.01, 0.99),
  na.rm = T
)

qqnorm(y = Win_cbrt_x, ylab = x)
qqline(y = Win_cbrt_x, ylab = x)

Win_raw_x = Winsorize(
  x = df$WoodDeckSF,
  probs = c(0, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'WoodDeckSF')
qqline(y = Win_raw_x, ylab = 'WoodDeckSF')

print(shapiro.test(x = df$WoodDeckSF))
print(shapiro.test(x = df[[x]]))
print(shapiro.test(x = Win_cbrt_x))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(cbrt(WoodDeckSF))' = ifelse(
      WoodDeckSF == 0,
      0,
      Winsorize(
        WoodDeckSF^(1/3),
        minval = min(Win_cbrt_x),
        maxval = max(Win_cbrt_x)
      )
    )
  ) %>%
  mutate(
    'Win(WoodDeckSF)' = ifelse(
      WoodDeckSF == 0,
      0,
      Winsorize(
        WoodDeckSF,
        minval = min(Win_raw_x),
        maxval = max(Win_raw_x)
      )
    )
  )
```

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('WoodDeck.bin' = ifelse(WoodDeckSF == 0, 0, 1)) %>%
  mutate('WoodDeck.bin.fact' = factor(WoodDeck.bin, ordered = T))

x = 'WoodDeck.bin.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('WoodDeckSF', 'cbrt(WoodDeckSF)', 'Win(cbrt(WoodDeckSF))',
          'Win(WoodDeckSF)', 'WoodDeck.bin')
x = 'Win(cbrt(WoodDeckSF))'

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('Win(log(SalePrice))')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
  # plot_scat_pairs(
  #   df = filter(val_train_Xy, WoodDeckSF != 0),
  #   x = feat,
  #   y_lst = y_lst
  # )
}
```

There's no real correlation between deck square footage and the target price when you remove houses without decks; the binary version does most of the work. But, the transformation does it a little better and does offer more normalized distance between points for KNN.

### Hard Code

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Win(cbrt(WoodDeckSF))'

min_val = min(Win_cbrt_x)
max_val = max(Win_cbrt_x)
print(paste("min_val:", min_val))
print(paste("max_val:", max_val))

# Already hard coded above

val_train_Xy = select(
  val_train_Xy,
  -c('WoodDeck.bin', 'WoodDeck.bin.fact', 'Win(WoodDeckSF)')
)

ggplot(val_train_Xy, aes(x = .data[[x]])) +
  geom_histogram(binwidth = .25)
```

## OpenPorchSF 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'OpenPorchSF'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$OpenPorchSF != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 10,
  t_binw = 0.1
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(OpenPorchSF)'
val_train_Xy = val_train_Xy %>%
  mutate('cbrt(OpenPorchSF)' = OpenPorchSF^(1/3))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy[[x]] != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.1,
  t_binw = 0.1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = filter(val_train_Xy, OpenPorchSF != 0)
x = 'cbrt(OpenPorchSF)'

qqnorm(y = df$OpenPorchSF, ylab = 'OpenPorchSF')
qqline(y = df$OpenPorchSF, ylab = 'OpenPorchSF')

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

Win_cbrt_x = Winsorize(
  x = df[[x]],
  probs = c(0.001, 0.994),
  na.rm = T
)

qqnorm(y = Win_cbrt_x, ylab = x)
qqline(y = Win_cbrt_x, ylab = x)

Win_raw_x = Winsorize(
  x = df$OpenPorchSF,
  probs = c(0, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'OpenPorchSF')
qqline(y = Win_raw_x, ylab = 'OpenPorchSF')

print(shapiro.test(x = df$OpenPorchSF))
print(shapiro.test(x = df[[x]]))
print(shapiro.test(x = Win_cbrt_x))
print(shapiro.test(x = Win_raw_x))
```
```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(cbrt(OpenPorchSF))' = ifelse(
      OpenPorchSF == 0,
      0,
      Winsorize(
        OpenPorchSF^(1/3),
        # probs = c(0.001, 0.994),
        # na.rm = T
        minval = min(Win_cbrt_x),
        maxval = max(Win_cbrt_x)
      )
    )
  )
```

Looks like some polymodality happening. Year built doesn't seem to explain it. I'm not going to manually search for it any further, but a decision tree or other ML algorithm may find and "factor" in the hidden interaction.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  filter(val_train_Xy, OpenPorchSF != 0),
  aes(x = `cbrt(OpenPorchSF)`)
) +
  geom_histogram(binwidth = 0.1) +
  facet_wrap(facets = vars(YearBuilt.fact), ncol = 1)

ggplot(
  filter(val_train_Xy, OpenPorchSF != 0),
  aes(x = `cbrt(OpenPorchSF)`, y = `log(SalePrice)`)
) +
  geom_point(aes(color = YearBuilt))
```

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('OpenPorch.bin' = ifelse(OpenPorchSF == 0, 0, 1)) %>%
  mutate('OpenPorch.bin.fact' = factor(OpenPorch.bin, ordered = T))

x = 'OpenPorch.bin.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('OpenPorchSF', 'cbrt(OpenPorchSF)', 'Win(cbrt(OpenPorchSF))',
          'OpenPorch.bin')
x = 'Win(cbrt(OpenPorchSF))'

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('Win(log(SalePrice))')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
  plot_scat_pairs(
    df = val_train_Xy[val_train_Xy[[x]] != 0, ],
    x = feat,
    y_lst = y_lst
  )
}
```

In this case, the binary variable is doing all the work.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy,
  -c('cbrt(OpenPorchSF)', 'Win(cbrt(OpenPorchSF))', 'OpenPorch.bin.fact')
)
```

## EnclosedPorch 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'EnclosedPorch'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$EnclosedPorch != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 5,
  t_binw = 5
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = filter(val_train_Xy, EnclosedPorch != 0)

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]], ylab = x)

Win_raw_x = Winsorize(
  x = df$EnclosedPorch,
  probs = c(0.001, 0.999),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x, ylab = 'Win_raw_x')

print(shapiro.test(x = df[[x]]))
print(shapiro.test(x = Win_raw_x))
```


### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('EnclosedPorch.bin' = ifelse(EnclosedPorch == 0, 0, 1)) %>%
  mutate(
    'EnclosedPorch.bin.fact' = factor(
      EnclosedPorch.bin,
      ordered = T
    )
  )

x = 'EnclosedPorch.bin.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('EnclosedPorch', 'EnclosedPorch.bin')
x = 'EnclosedPorch'

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = c(x),
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('log(SalePrice)')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
  plot_scat_pairs(
    df = val_train_Xy[val_train_Xy[[x]] != 0, ],
    x = feat,
    y_lst = y_lst
  )
}
```

Houses with enclosed porches are significantly cheaper than those without, maybe due to a confounding variable like year built. There is a weak, if existent, positive correlation between square footage and price within those that have them.

```{r echo=TRUE, message=FALSE, warning=FALSE}
fenced_jbv(
  data = val_train_Xy,
  x = 'EnclosedPorch.bin.fact',
  y = 'log(SalePrice)',
  jit_col = 'YearBuilt',
  jit_alpha = 1
) +
  facet_wrap(facets = vars(YearBuilt.fact))

val_train_Xy$resids = lm(
  `log(SalePrice)` ~ `sqrt(Age)`,
  val_train_Xy
)$residuals

ggplot(val_train_Xy, aes(x = EnclosedPorch, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy, -c('EnclosedPorch.bin', 'EnclosedPorch.bin.fact')
)
```

## X3SsnPorch

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$X3SsnPorch)
val_train_Xy = select(val_train_Xy, -c('X3SsnPorch'))
```

## ScreenPorch

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'ScreenPorch'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$ScreenPorch != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 10,
  t_binw = 0.1
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(ScreenPorch)'
val_train_Xy = val_train_Xy %>%
  mutate('cbrt(ScreenPorch)' = ScreenPorch^(1/3))

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$`cbrt(ScreenPorch)` != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 0.1,
  t_binw = 0.1
)
```

### Winsorize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'cbrt(ScreenPorch)'
df = filter(val_train_Xy, ScreenPorch != 0) 

qqnorm(y = df$ScreenPorch, ylab = 'ScreenPorch')
qqline(y = df$ScreenPorch)

qqnorm(y = df[[x]], ylab = x)
qqline(y = df[[x]])

Win_cbrt_x = Winsorize(
  df[[x]],
  probs = c(0.05, 0.95),
  na.rm = T
)

qqnorm(y = Win_cbrt_x, ylab = 'Win_cbrt_x')
qqline(y = Win_cbrt_x)

Win_raw_x = Winsorize(
  df$ScreenPorch,
  probs = c(0.05, 0.95),
  na.rm = T
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x)

print(shapiro.test(x = df$ScreenPorch))
print(shapiro.test(x = df[[x]]))
print(shapiro.test(x = Win_cbrt_x))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(cbrt(ScreenPorch))' = ifelse(
      ScreenPorch == 0,
      0,
      Winsorize(
        ScreenPorch^(1/3),
        minval = min(Win_cbrt_x),
        maxval = max(Win_cbrt_x)
      )
    )
  )
```

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('ScreenPorch.bin' = ifelse(ScreenPorch ==0, 0, 1)) %>%
  mutate('ScreenPorch.bin.fact' = factor(ScreenPorch.bin, ordered = T))

x = 'ScreenPorch.bin.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('ScreenPorch', 'cbrt(ScreenPorch)', 'Win(cbrt(ScreenPorch))',
          'ScreenPorch.bin')
x = 'Win(cbrt(ScreenPorch))'

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('Win(log(SalePrice))')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
  plot_scat_pairs(
    df = val_train_Xy[val_train_Xy[[x]] != 0, ],
    x = feat,
    y_lst = y_lst
  )
}
```

The binary doesn't seem to make a significant difference as other binaries like WoodDeck and OpenPorch. So, it's not worth using alone, but might be useful in interaction with the area in a linear regression. Decision trees should be able to do without the binary feature. The transformation may help KNN, but like a decision tree, I would want to avoid overweighting by including both.

### "Controlling"

```{r echo=TRUE, message=FALSE, warning=FALSE}
fenced_jbv(
  data = val_train_Xy,
  x = 'ScreenPorch.bin.fact',
  y = 'log(SalePrice)',
  jit_col = 'YearBuilt',
  jit_alpha = 1
) +
  facet_wrap(facets = vars(YearBuilt.fact))

val_train_Xy$resids = lm(
  `log(SalePrice)` ~ `sqrt(Age)`,
  val_train_Xy
)$residuals

ggplot(val_train_Xy, aes(x = `cbrt(ScreenPorch)`, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm')

print("Correlation of cbrt(ScreenPorch) to residuals of `log(SalePrice)` ~ `sqrt(Age)`")
print(cor(x = val_train_Xy$`cbrt(ScreenPorch)`, y = val_train_Xy$resids))

df = filter(val_train_Xy, ScreenPorch != 0)

ggplot(df, aes(x = `cbrt(ScreenPorch)`, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm')

print("Exluding ScreenPorch 0s:")
print(cor(x = val_train_Xy$`cbrt(ScreenPorch)`, y = val_train_Xy$resids))
```

This feature doesn't seem to have much to offer. But, I'll leave it anyway and let feature selection during modeling suss that out.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy, -c('ScreenPorch.bin.fact', 'Win(cbrt(ScreenPorch))')
)
```

## PoolArea, QC

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(val_train_Xy$PoolQC)
val_train_Xy = select(val_train_Xy, -c('PoolArea', 'PoolQC'))
```

## Fence

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'Fence'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('Fence.bin' = ifelse(Fence == 'None', 0, 1)) %>%
  mutate('Fence.bin.fact' = factor(Fence.bin, ordered = T))

x = 'Fence.bin.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

Having a Fence seems to detract value, probably due to an interaction with another variable as we saw with EnclosedPorch and age. Unlike EnclosedPorch, it's not immediately obvious what the other variable is, and "controlling" for it with a linear regression may actually increase Fence's significance. So, rather than hunting for the other variable(s) or dropping this one, I'll leave it and see if ML modeling can make use of it.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy,
  -c('Fence.bin', 'Fence.bin.fact')
)
```

## MiscFeature, MiscVal

[Back to top.](#top)

MiscVal is kind of a cheater variable. It should have precisely 1 for its coefficient. Otherwise, I would just drop it for so few observations; it might just throw of the regression. If I keep it, it should be transformed in the same way that the target variable is.

It also looks like the presence of a miscellaneous improvement is associated with a lower price if anything.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'MiscFeature'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## MiscVal 

[Back to top.](#top)

### Normalize

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'MiscVal'
summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$MiscVal != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = 200,
  t_binw = .1
)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'log2(MiscVal)'
val_train_Xy = val_train_Xy %>%
  mutate(
    'log2(MiscVal)' = ifelse(
      MiscVal == 0,
      0,
      log2(MiscVal)
    )
  )

# Recalculate best normalizers.
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
best_normalizers = find_best_normalizer_per_feat(
  df = val_train_Xy,
  feats_lst = num_feats,
  funcs_lst = funcs_lst,
  exclude_vals = list(0)
)

summary(val_train_Xy[x])
sum_and_trans_cont(
  data = val_train_Xy[val_train_Xy$`log2(MiscVal)` != 0, ],
  x = x,
  func = best_normalizers[[x]]$best_func$func,
  func_name = best_normalizers[[x]]$best_func$name,
  x_binw = .1,
  t_binw = .1
)
```

Alright, I'll give it a natural log like SalePrice, since it is a straight dollar value.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'log(MiscVal)'
val_train_Xy = val_train_Xy %>%
  mutate(
    'log(MiscVal)' = ifelse(
      MiscVal == 0,
      0,
      log(MiscVal)
    )
  )

df = filter(val_train_Xy, MiscVal != 0)

gg = ggplot(df, aes(x = `log(MiscVal)`))
p1 = gg + geom_histogram(binwidth = 0.1)
p2 = gg + geom_boxplot(notch = T)
grid.arrange(p1, p2)
```

### Winsorize

Since this variable is an actual dollar value, Winsorizing it doesn't really make sense. I'll check it out anyway.

```{r echo=TRUE, message=FALSE, warning=FALSE}
qqnorm(y = df$MiscVal, ylab = 'MiscVal')
qqline(y = df$MiscVal)

qqnorm(y = df$`log(MiscVal)`, ylab = 'log(MiscVal)')
qqline(y = df$`log(MiscVal)`)

Win_log_x = Winsorize(
  df$`log(MiscVal)`,
  probs = c(0.007, 0.95),
  na.rm = T
)

qqnorm(y = Win_log_x, ylab = 'Win_log_x')
qqline(y = Win_log_x)

Win_raw_x = Winsorize(
  df$MiscVal,
  probs = c(0, 0.95)
)

qqnorm(y = Win_raw_x, ylab = 'Win_raw_x')
qqline(y = Win_raw_x)

print(shapiro.test(x = df$MiscVal))
print(shapiro.test(x = df$`log(MiscVal)`))
print(shapiro.test(x = Win_log_x))
print(shapiro.test(x = Win_raw_x))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    'Win(log(MiscVal))' = ifelse(
      MiscVal == 0,
      0,
      Winsorize(
        log(MiscVal),
        minval = min(Win_log_x),
        maxval = max(Win_log_x)
      )
    )
  )
```

### Binarize

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('MiscVal.bin' = ifelse(MiscVal == 0, 0, 1)) %>%
  mutate('MiscVal.bin.fact' = factor(MiscVal.bin, ordered = T))

x = 'MiscVal.bin.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

### Correlations

```{r echo=TRUE, message=FALSE, warning=FALSE}
num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('MiscVal', 'log2(MiscVal)', 'log(MiscVal)',
          'Win(log(MiscVal))', 'MiscVal.bin')

x = 'log(MiscVal)'

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    !is.na(.data[[x]])
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs:")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features')

df = get_cors(
  data = filter(
    select(val_train_Xy, all_of(num_feats)),
    .data[[x]] != 0
  ),
  x_lst = x_lst,
  feats = num_feats
)
df
print("Summary of absolute values of Pearson's Rs (no 0s):")
df = abs(df)
summary(abs(df))

df = melt(df)
ggplot(df, aes(x = variable, y = value)) +
  geom_boxplot(notch = T) +
  ylab(label = 'Absolute Value of Correlation to Other Features (no 0s)')

y_lst = c('Win(log(SalePrice))')
for (feat in x_lst) {
  plot_scat_pairs(df = val_train_Xy, x = feat, y_lst = y_lst)
  plot_scat_pairs(
    df = val_train_Xy[val_train_Xy[[x]] != 0, ],
    x = feat,
    y_lst = y_lst
  )
}
```

### "Controlling"

MiscVal appears to be correlated with the size of the lot and house. Perhaps that will do the heavy lifting and make MiscVal obsolete.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy$resids = lm(
  `log(SalePrice)` ~ `Win(LotArea)`,
  val_train_Xy
)$residuals

print("Correlation of log(MiscVal) to residuals of `log(SalePrice)` ~ `Win(LotArea)`")
print(cor(x = val_train_Xy$`log(MiscVal)`, y = val_train_Xy$resids))

ggplot(val_train_Xy, aes(x = `log(MiscVal)`, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  ylab(label = "`log(SalePrice)` ~ `Win(LotArea)`")

df = filter(val_train_Xy, MiscVal != 0)

print("Exluding MiscVal 0s:")
print(cor(x = df$`log(MiscVal)`, y = df$resids))

ggplot(df, aes(x = `log(MiscVal)`, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  ylab(label = "`log(SalePrice)` ~ `Win(LotArea)`")


val_train_Xy$resids = lm(
  `log(SalePrice)` ~ `log10(log10(LotArea))`,
  val_train_Xy
)$residuals

print("Correlation of log(MiscVal) to residuals of `log(SalePrice)` ~ `log10(log10(LotArea))`")
print(cor(x = val_train_Xy$`log(MiscVal)`, y = val_train_Xy$resids))

ggplot(val_train_Xy, aes(x = `log(MiscVal)`, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  ylab(label = "`log(SalePrice)` ~ `log10(log10(LotArea))`")

df = filter(val_train_Xy, MiscVal != 0)

print("Exluding MiscVal 0s:")
print(cor(x = df$`log(MiscVal)`, y = df$resids))

ggplot(df, aes(x = `log(MiscVal)`, y = resids)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  ylab(label = "`log(SalePrice)` ~ `log10(log10(LotArea))`")
```

After factoring in Lot Area, there's still some correlation between MiscVal and the target once 0s are removed. It may still be worth keeping. We'll let modeling suss that out.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = select(
  val_train_Xy, -c('MiscVal.bin', 'MiscVal.bin.fact', 'log2(MiscVal)')
  )
```

## MoSold, YrSold

[Back to top.](#top)

MoSold somewhat normally distributed around June/July. Conventional wisdom says that summer sales are higher in volume and price, but the data don't bear that out for price.

Records go from January 2006 through July 2010, so August-December are underrepresented by roughly 20%.

YrSold pretty uniform (about 140-160) except for 2010 which ended in July in this set.

Interesting spike in sales in Spring 2010. Foreclosures coming onto market?

### Set to Factors: MoSold, YrSold

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate('MoSold.fact' = factor(MoSold)) %>%
  mutate('YrSold.fact' = factor(YrSold, ordered = T))

x = 'MoSold.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)


x = 'YrSold.fact'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

## SoldDate

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy = val_train_Xy %>%
  mutate(
    SoldDate = as.Date(
      paste(
        as.character(YrSold),
        as.character(MoSold),
        '15',
        sep = '/'
      ),
      format = '%Y/%m/%d'
    )
  )

ggplot(val_train_Xy, aes(x = SoldDate)) +
  geom_bar()

x = 'SoldDate'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)


num_feats = colnames(select(val_train_Xy, where(is.numeric)))
x_lst = c('SoldDate')
# 
# df = get_cors(
#   data = filter(
#     select(val_train_Xy, all_of(num_feats)),
#     !is.na(.data[[x]])
#   ),
#   x_lst = x_lst,
#   feats = num_feats
# )
# df
# print("Summary of absolute values of Pearson's Rs:")
# df = abs(df)
# summary(abs(df))
# 
# df = melt(df)
# ggplot(df, aes(x = variable, y = value)) +
#   geom_boxplot(notch = T) +
#   ylab(label = 'Absolute Value of Correlation to Other Features')

y_lst = c('log(SalePrice)')
plot_scat_pairs(df = val_train_Xy, x = x, y_lst = y_lst)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(
  val_train_Xy,
  aes(x = SoldDate, y = `log(SalePrice)`)
) +
  geom_point() +
  geom_smooth() +
  geom_smooth(method = 'lm', color = 'Yellow') +
  facet_grid(rows = vars(SaleCondition), cols = vars(SaleType))
```

None of date seems to matter in this set. Drop it, but no yet.

## SaleType

[Back to top.](#top)

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'SaleType'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```

<a id="saletypeDate"></a>

```{r echo=TRUE, message=FALSE, warning=FALSE}
df = val_train_Xy[val_train_Xy$SaleType %in% c('WD', 'New', 'COD'), ]
gg = ggplot(df, aes(x = SaleType))

gg + geom_bar() +
  facet_grid(rows = vars(MoSold), cols = vars(YrSold)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

gg + geom_bar(position = 'dodge', aes(fill = factor(YrSold)))

ggplot(df, aes(color = SaleType, x = SoldDate)) +
  geom_freqpoly()

# ggplot(df, aes(x = SaleType, y = SalePrice)) +
#   geom_col(position = 'dodge', aes(fill = factor(YrSold), stat = 'mean'))

ggplot(df, aes(x = SaleType, y = `log(SalePrice)`)) +
  geom_boxplot(position = 'dodge', notch = T, aes(color = factor(YrSold)))
```

You can see new home sales drop off with the crash. Court officer deeds also ticked up, possibly due to more foreclosures. But, there didn't seem to be a significant difference in sale prices year over year within sale type groups, except between new sales in 2007and 2010 which is not fully represented in this set.

## SaleCondition

[Back to top.](#top)

592 normal, 61 partial (home not completed when last assessed), 43 abnormal (trade, foreclosure, short sale), 11 family. I'm guessing a family sale will be lower in price typically, as will foreclosures and shortsales. I'm not sure what to make of partials; the house wasn't fully built when assessed, so the price may be askew?

Surprisingly, abnormal sales didn't seem to vary with the crash. Ames appears to have fared well.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x = 'SaleCondition'
y = 'SalePrice'
summarize_by(data = val_train_Xy, x = x, y = y)
y = 'log(SalePrice)'
sum_and_trans_fact(data = val_train_Xy, x = x, y = y)

p_vals = get_signif_levels(data = val_train_Xy, x = x, z = y, min_n = 30)

heatmap.2(
    x = as.matrix(p_vals$pval_df),
    scale = 'none',
    Rowv = F,
    Colv = F,
    dendrogram = 'none',
    cellnote = format(p_vals$pval_df, digits = 2),
    notecex = 0.75,
    notecol = 'black',
    main = paste(y, 'p-values'),
    key = F
  )

print(
    paste(
      "Levels w/ significantly different",
      y,
      "than another level:"
    )
  )
print(p_vals$signif_levels)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
df = filter(val_train_Xy, SaleCondition %in% c('Normal', 'Abnorml', 'Partial'))

gg = ggplot(df, aes(x = SaleCondition))

gg + geom_bar() +
  facet_grid(rows = vars(MoSold), cols = vars(YrSold)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

gg + geom_bar(aes(fill = factor(YrSold)), position = 'dodge')

ggplot(df, aes(x = SoldDate, color = SaleCondition)) +
  geom_freqpoly()

ggplot(df, aes(x = SaleCondition, y = `log(SalePrice)`)) +
  geom_boxplot(position = 'dodge', notch = T, aes(color = factor(YrSold)))
```

Bearing in mind that 2010 is not a complete year in this set, partial sales dropped as normal sales increased. This trend may be explained by developers finishing/halting their projects. Filtering/grouping by year built and/or neighborhood might help check this, but I'll skip it for the sake of finishing.

Fall and winter months seemed to be where the bulk of these increases in normal sales fell each year.


## Overall Correlations

[Back to top.](#top)

To wrap up this notebook, and as a preliminary gauge on how well I have prepared the data for ML, mainly for linear regression, I'll check out how correlations have changed. Have my variables increased in correlation in general? Have they increased in correlation to the target variable?

Increased correlations to the target variable have obvious benefits. Increased correlations between predictor variables will help clarify which variables may overlap in their predictive power and redundantly overweight the same underlying information.

I made a lot of new features, dropping some along the way. In cases where a Winsorized version seemed a good option, I also kept the scale-transformed version where applicable. For example, I kept both log(SalePrice) and Win(log(SalePrice)). Winsorization will help a straightforward linear regression without interactions. But, Winsorization will undercut KNN's ability to cluster multivariate outliers and similarly RF's ability to group by extremes. That said, Winsorization will reduce the chances of overfit in all three. All that is to say that there are redundant new features.

I can't think of a quick and dirty way to view this without getting skewed results or busy heatmaps, other than to make a table of all the variables' correlations to the target variable which isn't very easily digested either. The slow and tedious way would be to iteratively "manually" (to some extent) select like variables for correlation within their experimental group. I'm not goin to do that.

```{r echo=TRUE, message=FALSE, warning=FALSE}
val_train_Xy_numeric = select(
  val_train_Xy, # Reorder for easier comparison.
  c('SalePrice', 'log(SalePrice)', 'Win(log(SalePrice))', "LotFrontage",
    "log10(LotFrontage)", "Win(log10(LotFrontage))", "Win(LotFrontage)",
    "LotArea", "log10(log10(LotArea))", "Win(LotArea)", "OverallQual_int",
    "OverallCond_int", "YearBuilt", "sqrt(Age)", "YearRemodAdd", "MasVnrArea",
    "cbrt(MasVnrArea)", "Win(cbrt(MasVnrArea))", "BsmtFinSF1", "BsmtFinSF2",
    "BsmtUnfSF", "cbrt(BsmtUnfSF)", "square(log(TotalBsmtSF))",
    "Win(square(log(TotalBsmtSF)))", "TotalBsmtSF", "TotalBsmtFinSF",
    "sqrt(TotalBsmtFinSF)", "Win(sqrt(TotalBsmtFinSF))", "X2ndFlrSF",
    "X2ndFlr.bin", "GrLivArea", "square(log2(GrLivArea))", 
    "Win(square(log2(GrLivArea)))", "TotBaths", "Win(TotBaths)",
    "BedroomAbvGr", "Win(BedroomAbvGr)", "TotRmsAbvGrd", "Win(BedroomAbvGr)",
    "TotRmsAbvGrd", "Win(TotRmsAbvGrd)", "Fireplaces.bin", "GarageCars",
    "Win(GarageCars)", "WoodDeckSF", "cbrt(WoodDeckSF)",
    "Win(cbrt(WoodDeckSF))", "OpenPorchSF", "OpenPorch.bin", "EnclosedPorch",
    "ScreenPorch", "cbrt(ScreenPorch)", "ScreenPorch.bin", "MiscVal",
    "log(MiscVal)", "Win(log(MiscVal))", "MoSold", "YrSold")
)

ggcorr(val_train_Xy_numeric)

cor_mtx = cor(val_train_Xy_numeric, use = 'pairwise.complete.obs')

target_vars_vec = c('SalePrice', 'log(SalePrice)', 'Win(log(SalePrice))')

cor_mtx_melted = melt(cor_mtx)
sales_cor_mtx_melted = filter(
  cor_mtx_melted,
  Var1 %in% target_vars_vec & !(Var2 %in% target_vars_vec)
)

ggplot(sales_cor_mtx_melted, aes(x = Var1, y = Var2)) +
  geom_tile(aes(fill = value))

dcast(sales_cor_mtx_melted, formula = Var2 ~ Var1)

fenced_jbv(
  data = sales_cor_mtx_melted,
  x = 'Var1',
  y = 'value',
  jit_h = 0
)
```

# Write to File

[Back to top.](#top)

I'll write it to a CSV file so I can verify that my final engineering script duplicates this process. I'll verify in the next notebook before I start modeling.

```{r echo=TRUE, warning=FALSE, message=FALSE}
val_train_Xy$Id = val_train_X$Id

saveRDS(val_train_Xy, 'data/eda_val_train_Xy.rds')
head(readRDS('data/eda_val_train_Xy.rds'))
```
